{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本セクションの目次\n",
    "1. データラングリングとは？\n",
    "2. テーブル形式を含むExcelのラングリング\n",
    "3. テーブル形式を含まないExcelのラングリング\n",
    "4. PDFのラングリングを行ってみよう\n",
    "5. ラングリングで気をつけること"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンソールで設定したSparkとNoteBookを接続します(動かす前に毎度実行する必要があります)\n",
    "import findspark\n",
    "findspark.init(\"/home/pyspark/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-streaming_2.13:3.2.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1,org.apache.spark:spark-avro_2.12:3.2.1\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# パッケージを複数渡したい時は「,」で繋いで渡します。\n",
    "# Sparkのバージョンにしっかりと合わせます(今回はSparkのバージョンが3.2を使っています。)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データラングリングとは？\n",
    "データラングリングとは、データをこねくり回してデータをより使いやすくする作業のことを指します。\n",
    "\n",
    "- 重複削除\n",
    "- idから商品名を引っ張っってくる\n",
    "- 使い物になる様に別テーブルとくっつける\n",
    "\n",
    "データラングリングと呼ばれる対象は一般にはCSV、JSON、アクセスログもあるのですが、それ以外にもExcelのデータ、PDFのデータ\n",
    "なども含まれています。\n",
    "\n",
    "最終的な目標はテーブルの形式にするためにどの様にロジックを組むのか？というところに落ち着いてきます。\n",
    "\n",
    "データラングリングというとかっこよく聞こえるかもしれないのですが、かなり地味な点と、エンジニアとしてラングリングを扱うには注意点がありますので  \n",
    "その点について紹介をしていこうと思います。\n",
    "\n",
    "データラングリングはPythonとPySparkを組み合わせながら進めていくことが多いです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テーブル形式を含むExcelのラングリング\n",
    "Excelのラングリングは、Sparkで読み込みをすることができません。\n",
    "そのためpandasを使ってExcelデータを読み込み、Sparkで処理をするということやってみたいと思います。\n",
    "\n",
    "Excelのデータは比較的小さいので操作はPandasで行ってもいいのですが、今回はSparkで処理を行ってみたいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('./dataset/table_excel.xlsx')\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テーブル形式を含まないExcelのラングリング\n",
    "\n",
    "お次はテーブルっぽくないexcelのラングリングをしてみましょう。  \n",
    "しかし心配は入りません。\n",
    "\n",
    "Excelであればいつでも単純に処理をすることが可能です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('./dataset/no_table_excel.xlsx')\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#スキーマ設定をしていきましょう\n",
    "struct = StructType([\n",
    "    StructField(\"1\", StringType(), False),\n",
    "    StructField(\"2\", StringType(), False),\n",
    "    StructField(\"koumoku\", StringType(), False),\n",
    "    StructField(\"val\", StringType(), False),\n",
    "    StructField(\"kesssai\", StringType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "])\n",
    "\n",
    "\n",
    "exceldf=spark.createDataFrame(df,schema=struct)\n",
    "exceldf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "columns = exceldf.columns\n",
    "for column in columns:\n",
    "    exceldf = exceldf.withColumn(column,F.when(F.isnan(F.col(column)),None).otherwise(F.col(column)))\n",
    "\n",
    "exceldf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceldf=exceldf.dropDuplicates().select(exceldf.koumoku,exceldf.val,exceldf.kesssai,exceldf.name)\n",
    "exceldf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceldf=exceldf.dropna(how='all')\n",
    "exceldf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceldf.withColumn('koumoku',F.when(exceldf.koumoku.isNull(),exceldf.kesssai).otherwise(exceldf.koumoku)).show()\n",
    "exceldf.withColumn('val',F.when(exceldf.val.isNull(),exceldf.name).otherwise(exceldf.val)).show()\n",
    "\n",
    "result=exceldf.withColumn('koumoku',F.when(exceldf.koumoku.isNull(),exceldf.kesssai).otherwise(exceldf.koumoku))\n",
    "result=result.withColumn('val',F.when(exceldf.val.isNull(),exceldf.name).otherwise(exceldf.val))\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=result.select(result.koumoku,result.val)\n",
    "spark.createDataFrame(result.toPandas().set_index('koumoku').T).show()\n",
    "\n",
    "\n",
    "result=spark.createDataFrame(result.toPandas().set_index('koumoku').T)\n",
    "\n",
    "# これでやっと既存のテーブルなどと突合したりができる様になってきます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDFのラングリングを行ってみよう\n",
    "PDFのラングリングは要注意です。\n",
    "基本的にできることはできるのですが、出力したPDFの作り方によってはまともに読めないことがあります。\n",
    "\n",
    "そのため、PDFのデータ解析をしたい！\n",
    "という要望を受けたら、基本的には断りつつExcelに変更してもらうなどの対応をとる方が賢明です。\n",
    "\n",
    "とはいえ、元のデータが残っておらずどうしてもやらなければならない時があるのでその時のために少しだけ方法を見てみましょう。\n",
    "有効な方法は以下の２です。\n",
    "\n",
    "- OCRでデータを読み取る(PyOCRなど)\n",
    "- ガッツリデータを読み込む\n",
    "\n",
    "今回はガッツリデータを読み込む方法で行ってみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import split\n",
    "from pdfminer.high_level import extract_text\n",
    "import re\n",
    "import os\n",
    "from decimal import Decimal\n",
    "\n",
    "text = extract_text(os.path.join(\"./dataset\", \"no_table_pdf.pdf\"))\n",
    "\n",
    "lines=text.split('\\n')\n",
    "\n",
    "#空行削除\n",
    "lines = list(filter(None, lines))\n",
    "\n",
    "for line in lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# あとは表示されたアウトプットをもとに整形をしていくだけです\n",
    "\n",
    "dict={}\n",
    "dict[lines[0]]=lines[2]\n",
    "dict[lines[1]]=lines[3]\n",
    "dict[lines[4]]=lines[5]\n",
    "\n",
    "print(dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dict=pd.DataFrame.from_dict(dict,orient='index')\n",
    "\n",
    "print(pd_dict)\n",
    "print(\"-------------\")\n",
    "print(pd_dict.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_spark=spark.createDataFrame(pd_dict.transpose()) \n",
    "pdf_spark.printSchema()\n",
    "pdf_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データラングリングで気をつけること\n",
    "\n",
    "ここまでみてどうだったでしょうか？\n",
    "基本的にはできそうだけども。。\n",
    "\n",
    "というところだったかと思います。\n",
    "\n",
    "基本的に既に稼働しているアプリケーションは、データ分析を前提に作られていることはないのでこの様な作業が発生してしまいます。\n",
    "\n",
    "そのため、必要であれば当然行うのですができる限りRDSなどの処理に落ち着ける様にできると良いかと思います。\n",
    "\n",
    "特にPDFは沼にハマることが多いので、最低でもExcelなどに落ち着ける様に調整を行いましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "578f5f657c2fb65ecadb997ad60e5cf2da380ecec34305a6dd913dc5b96e257c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
