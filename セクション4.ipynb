{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本セクションの目次\n",
    "1. Avroフォーマット\n",
    "2. 前方互換と後方互換と完全互換\n",
    "3. メッセージキューとAvroを連携してみよう\n",
    "4. Avroファイルの読み書き\n",
    "5. Avroで前方互換をやってみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンソールで設定したSparkとNoteBookを接続します(動かす前に毎度実行する必要があります)\n",
    "import findspark\n",
    "findspark.init(\"/home/pyspark/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition.mode\n",
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/pyspark/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/pyspark/spark-3.2.0-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/pyspark/.ivy2/cache\n",
      "The jars for the packages stored in: /home/pyspark/.ivy2/jars\n",
      "org.apache.spark#spark-streaming_2.13 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d68288ce-a580-4654-9cb6-d666e43d44f7;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.2.0 in central\n",
      "\tfound org.tukaani#xz;1.8 in central\n",
      ":: resolution report :: resolve 346ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.tukaani#xz;1.8 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   15  |   0   |   0   |   0   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d68288ce-a580-4654-9cb6-d666e43d44f7\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/7ms)\n",
      "22/01/05 13:15:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/05 13:15:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/01/05 13:15:14 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-streaming_2.13:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-avro_2.12:3.2.0\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# パッケージを複数渡したい時は「,」で繋いで渡します。\n",
    "# Sparkのバージョンにしっかりと合わせます(今回はSparkのバージョンが3.2を使っています。)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データラングリングとは？\n",
    "データラングリングとは、データをこねくり回してデータをより使いやすくする作業のことを指します。\n",
    "\n",
    "- 重複削除\n",
    "- idから商品名を引っ張っってくる\n",
    "- 使い物になる様に別テーブルとくっつける\n",
    "\n",
    "データラングリングと呼ばれる対象は一般にはCSV、JSON、アクセスログもあるのですが、それ以外にもExcelのデータ、PDFのデータ\n",
    "なども含まれています。\n",
    "\n",
    "最終的な目標はテーブルの形式にするためにどの様にロジックを組むのか？というところに落ち着いてきます。\n",
    "\n",
    "データラングリングというとかっこよく聞こえるかもしれないのですが、かなり地味な点と、エンジニアとしてラングリングを扱うには注意点がありますので  \n",
    "その点について紹介をしていこうと思います。\n",
    "\n",
    "データラングリングはPythonとPySparkを組み合わせながら進めていくことが多いです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テーブル形式を含むExcelのラングリング\n",
    "Excelのラングリングは、Sparkで読み込みをすることができません。\n",
    "そのためpandasを使ってExcelデータを読み込み、Sparkで処理をするということやってみたいと思います。\n",
    "\n",
    "Excelのデータは比較的小さいので操作はPandasで行ってもいいのですが、今回はSparkで処理を行ってみたいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hoge  peke\n",
      "0   1.0   2.0\n",
      "1   3.0   2.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('./dataset/table_excel.xlsx')\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テーブル形式を含まないExcelのラングリング\n",
    "\n",
    "お次はテーブルっぽくないexcelのラングリングをしてみましょう。  \n",
    "しかし心配は入りません。\n",
    "\n",
    "Excelであればいつでも単純に処理をすることが可能です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Unnamed: 1 Unnamed: 2  Unnamed: 3 Unnamed: 4 Unnamed: 5\n",
      "0         NaN         NaN        NaN         NaN        NaN        NaN\n",
      "1         NaN         NaN        NaN         NaN        NaN        NaN\n",
      "2         NaN         NaN         売上       100.0        NaN        NaN\n",
      "3         NaN         NaN        消費税        10.0        NaN        NaN\n",
      "4         NaN         NaN        NaN         NaN        NaN        NaN\n",
      "5         NaN         NaN        NaN         NaN        決済者   hogepeke\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('./dataset/no_table_excel.xlsx')\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+-----+-------+--------+\n",
      "|  1|  2|koumoku|  val|kesssai|    name|\n",
      "+---+---+-------+-----+-------+--------+\n",
      "|NaN|NaN|    NaN|  NaN|    NaN|     NaN|\n",
      "|NaN|NaN|    NaN|  NaN|    NaN|     NaN|\n",
      "|NaN|NaN|   売上|100.0|    NaN|     NaN|\n",
      "|NaN|NaN| 消費税| 10.0|    NaN|     NaN|\n",
      "|NaN|NaN|    NaN|  NaN|    NaN|     NaN|\n",
      "|NaN|NaN|    NaN|  NaN| 決済者|hogepeke|\n",
      "+---+---+-------+-----+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#スキーマ設定をしていきましょう\n",
    "struct = StructType([\n",
    "    StructField(\"1\", StringType(), False),\n",
    "    StructField(\"2\", StringType(), False),\n",
    "    StructField(\"koumoku\", StringType(), False),\n",
    "    StructField(\"val\", StringType(), False),\n",
    "    StructField(\"kesssai\", StringType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "])\n",
    "\n",
    "\n",
    "exceldf=spark.createDataFrame(df,schema=struct)\n",
    "exceldf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+-----+-------+--------+\n",
      "|   1|   2|koumoku|  val|kesssai|    name|\n",
      "+----+----+-------+-----+-------+--------+\n",
      "|null|null|   null| null|   null|    null|\n",
      "|null|null|   null| null|   null|    null|\n",
      "|null|null|   売上|100.0|   null|    null|\n",
      "|null|null| 消費税| 10.0|   null|    null|\n",
      "|null|null|   null| null|   null|    null|\n",
      "|null|null|   null| null| 決済者|hogepeke|\n",
      "+----+----+-------+-----+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "columns = exceldf.columns\n",
    "for column in columns:\n",
    "    exceldf = exceldf.withColumn(column,F.when(F.isnan(F.col(column)),None).otherwise(F.col(column)))\n",
    "\n",
    "exceldf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+--------+\n",
      "|koumoku|  val|kesssai|    name|\n",
      "+-------+-----+-------+--------+\n",
      "|   null| null|   null|    null|\n",
      "|   売上|100.0|   null|    null|\n",
      "| 消費税| 10.0|   null|    null|\n",
      "|   null| null| 決済者|hogepeke|\n",
      "+-------+-----+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exceldf=exceldf.dropDuplicates().select(exceldf.koumoku,exceldf.val,exceldf.kesssai,exceldf.name)\n",
    "exceldf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+--------+\n",
      "|koumoku|  val|kesssai|    name|\n",
      "+-------+-----+-------+--------+\n",
      "|   売上|100.0|   null|    null|\n",
      "| 消費税| 10.0|   null|    null|\n",
      "|   null| null| 決済者|hogepeke|\n",
      "+-------+-----+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exceldf=exceldf.dropna(how='all')\n",
    "exceldf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+--------+\n",
      "|koumoku|  val|kesssai|    name|\n",
      "+-------+-----+-------+--------+\n",
      "|   売上|100.0|   null|    null|\n",
      "| 消費税| 10.0|   null|    null|\n",
      "| 決済者| null| 決済者|hogepeke|\n",
      "+-------+-----+-------+--------+\n",
      "\n",
      "+-------+--------+-------+--------+\n",
      "|koumoku|     val|kesssai|    name|\n",
      "+-------+--------+-------+--------+\n",
      "|   売上|   100.0|   null|    null|\n",
      "| 消費税|    10.0|   null|    null|\n",
      "|   null|hogepeke| 決済者|hogepeke|\n",
      "+-------+--------+-------+--------+\n",
      "\n",
      "+-------+--------+-------+--------+\n",
      "|koumoku|     val|kesssai|    name|\n",
      "+-------+--------+-------+--------+\n",
      "|   売上|   100.0|   null|    null|\n",
      "| 消費税|    10.0|   null|    null|\n",
      "| 決済者|hogepeke| 決済者|hogepeke|\n",
      "+-------+--------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exceldf.withColumn('koumoku',F.when(exceldf.koumoku.isNull(),exceldf.kesssai).otherwise(exceldf.koumoku)).show()\n",
    "exceldf.withColumn('val',F.when(exceldf.val.isNull(),exceldf.name).otherwise(exceldf.val)).show()\n",
    "\n",
    "result=exceldf.withColumn('koumoku',F.when(exceldf.koumoku.isNull(),exceldf.kesssai).otherwise(exceldf.koumoku))\n",
    "result=result.withColumn('val',F.when(exceldf.val.isNull(),exceldf.name).otherwise(exceldf.val))\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+\n",
      "| 売上|消費税|  決済者|\n",
      "+-----+------+--------+\n",
      "|100.0|  10.0|hogepeke|\n",
      "+-----+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result=result.select(result.koumoku,result.val)\n",
    "spark.createDataFrame(result.toPandas().set_index('koumoku').T).show()\n",
    "\n",
    "\n",
    "result=spark.createDataFrame(result.toPandas().set_index('koumoku').T)\n",
    "\n",
    "# これでやっと既存のテーブルなどと突合したりができる様になってきます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDFのラングリングを行ってみよう\n",
    "PDFのラングリングは要注意です。\n",
    "基本的にできることはできるのですが、出力したPDFの作り方によってはまともに読めないことがあります。\n",
    "\n",
    "そのため、PDFのデータ解析をしたい！\n",
    "という要望を受けたら、基本的には断りつつExcelに変更してもらうなどの対応をとる方が賢明です。\n",
    "\n",
    "とはいえ、元のデータが残っておらずどうしてもやらなければならない時があるのでその時のために少しだけ方法を見てみましょう。\n",
    "有効な方法は以下の２です。\n",
    "\n",
    "- OCRでデータを読み取る(PyOCRなど)\n",
    "- ガッツリデータを読み込む\n",
    "\n",
    "今回はガッツリデータを読み込む方法で行ってみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "売上\n",
      "消費税\n",
      "100\n",
      "10\n",
      "決済者\n",
      "hogepeke\n",
      "\f\n"
     ]
    }
   ],
   "source": [
    "from re import split\n",
    "from pdfminer.high_level import extract_text\n",
    "import re\n",
    "import os\n",
    "from decimal import Decimal\n",
    "\n",
    "text = extract_text(os.path.join(\"./dataset\", \"no_table_pdf.pdf\"))\n",
    "\n",
    "lines=text.split('\\n')\n",
    "\n",
    "#空行削除\n",
    "lines = list(filter(None, lines))\n",
    "\n",
    "for line in lines:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'売上': '100', '消費税': '10', '決済者': 'hogepeke'}\n"
     ]
    }
   ],
   "source": [
    "# あとは表示されたアウトプットをもとに整形をしていくだけです\n",
    "\n",
    "dict={}\n",
    "dict[lines[0]]=lines[2]\n",
    "dict[lines[1]]=lines[3]\n",
    "dict[lines[4]]=lines[5]\n",
    "\n",
    "print(dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0\n",
      "売上        100\n",
      "消費税        10\n",
      "決済者  hogepeke\n",
      "-------------\n",
      "    売上 消費税       決済者\n",
      "0  100  10  hogepeke\n"
     ]
    }
   ],
   "source": [
    "pd_dict=pd.DataFrame.from_dict(dict,orient='index')\n",
    "\n",
    "print(pd_dict)\n",
    "print(\"-------------\")\n",
    "print(pd_dict.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 売上: string (nullable = true)\n",
      " |-- 消費税: string (nullable = true)\n",
      " |-- 決済者: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------+\n",
      "|売上|消費税|  決済者|\n",
      "+----+------+--------+\n",
      "| 100|    10|hogepeke|\n",
      "+----+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "pdf_spark=spark.createDataFrame(pd_dict.transpose()) \n",
    "pdf_spark.printSchema()\n",
    "pdf_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データラングリングで気をつけること\n",
    "\n",
    "ここまでみてどうだったでしょうか？\n",
    "基本的にはできそうだけども。。\n",
    "\n",
    "というところだったかと思います。\n",
    "\n",
    "基本的に既に稼働しているアプリケーションは、データ分析を前提に作られていることはないのでこの様な作業が発生してしまいます。\n",
    "\n",
    "そのため、必要であれば当然行うのですができる限りRDSなどの処理に落ち着ける様にできると良いかと思います。\n",
    "\n",
    "特にPDFは沼にハマることが多いので、最低でもExcelなどに落ち着ける様に調整を行いましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "578f5f657c2fb65ecadb997ad60e5cf2da380ecec34305a6dd913dc5b96e257c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
