{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本セクションの目次\n",
    "1. Avroフォーマット\n",
    "2. 前方互換と後方互換と完全互換\n",
    "3. メッセージキューとAvroを連携してみよう\n",
    "4. Avroファイルの読み書き\n",
    "5. Avroで前方互換をやってみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンソールで設定したSparkとNoteBookを接続します(動かす前に毎度実行する必要があります)\n",
    "import findspark\n",
    "findspark.init(\"/home/pyspark/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition.mode\n",
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/pyspark/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/pyspark/spark-3.2.0-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/pyspark/.ivy2/cache\n",
      "The jars for the packages stored in: /home/pyspark/.ivy2/jars\n",
      "org.apache.spark#spark-streaming_2.13 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a28822ed-5037-4a99-814c-64304dc0d58b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.2.0 in central\n",
      "\tfound org.tukaani#xz;1.8 in central\n",
      ":: resolution report :: resolve 640ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.tukaani#xz;1.8 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   15  |   0   |   0   |   0   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a28822ed-5037-4a99-814c-64304dc0d58b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/8ms)\n",
      "22/01/05 06:34:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-streaming_2.13:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-avro_2.12:3.2.0\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# パッケージを複数渡したい時は「,」で繋いで渡します。\n",
    "# Sparkのバージョンにしっかりと合わせます(今回はSparkのバージョンが3.2を使っています。)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ビッグデータの世界のDDL\n",
    "\n",
    "ビッグデータの世界でのDDLはRDSと同じ様にDDL文を実行することが可能です。  \n",
    "今回は以下のDDLについてみていきましょう  \n",
    "\n",
    "- Create Database 文\n",
    "- CREATE EXTERNAL TABLE文\n",
    "- CREATE VIEW\n",
    "- ADD PARTITION(MSCK REPAIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE DATABASE文\n",
    "データベースの作成を行います。\n",
    "こちらはRDSなどのCreate Database　と同じ方法で作成が可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/05 06:34:53 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "22/01/05 06:34:53 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "Wed Jan 05 06:34:54 UTC 2022 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.\n",
      "Wed Jan 05 06:34:54 UTC 2022 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.\n",
      "Wed Jan 05 06:34:54 UTC 2022 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.\n",
      "Wed Jan 05 06:34:54 UTC 2022 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.\n",
      "Wed Jan 05 06:34:54 UTC 2022 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.\n",
      "Wed Jan 05 06:34:54 UTC 2022 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.\n",
      "Wed Jan 05 06:34:54 UTC 2022 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.\n",
      "Wed Jan 05 06:34:54 UTC 2022 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.\n",
      "22/01/05 06:34:55 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "22/01/05 06:34:55 WARN ObjectStore: Failed to get database super_crush_course, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create database if not exists super_crush_course\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|namespace                   |\n",
      "+----------------------------+\n",
      "|data_management_crush_course|\n",
      "|default                     |\n",
      "|metadata_tmp                |\n",
      "|super_crush_course          |\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# データベースの一覧を見てみましょう\n",
    "spark.sql(\"show databases\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE EXTERNAL TABLE文\n",
    "テーブル定義の構成要素をみていきましょう\n",
    "\n",
    "```\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS super_crush_course.csv_table ( id INT, date STRING)\n",
    "PARTITIONED BY (dt INT)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LOCATION '/home/pyspark/super_crush_course.db/csv_table/dataset/parquet/';\n",
    "\n",
    "#S3などであれば、以下のように設定を変えることも可能です。\n",
    "LOCATION 's3://data.platform/super_crush_course.db/raw_zone/sampletable/';\n",
    "\n",
    "```\n",
    "\n",
    "ビッグデータの世界では、実データとデータベース定義/テーブル定義(メタデータ)は明確に分離されています。  \n",
    "今回のコースだと実データはローカル端末のSSD(HDD)でテーブル定義はMysqlに登録されています。  \n",
    "明確に分離されているからこそ、場所を指し示す宣言であるLocationが必要になってきます  \n",
    "\n",
    "ロケーションは「super_crush_course.db」とDB名.db/TABLE名とすることが通例です。  \n",
    "Externalは外部のという意味で、オンプレ環境の場合はつけないことが多かったが、クラウド環境ではつけるのが必須となっている設定。\n",
    "\n",
    "メタデータについてさらに詳しく知りたい方は、以下の記事を参照してみてください  \n",
    "「【PythonとSparkで始めるデータマネジメント入門】 ビッグデータレイクのための統合メタデータ管理入門」\n",
    "\n",
    "上記の例は、CSV形式のテーブルです。  \n",
    "それ以外にもParquet形式、Avro形式でテーブルを作成することができます。\n",
    "\n",
    "```\n",
    "# パーティションつきのテーブル\n",
    "CREATE TABLE IF NOT EXISTS super_crush_course.parquet_table \n",
    "(code String, gengo String,wareki String,seireki String,chu String,sokei String,jinko_male String,jinko_femail String)\n",
    "PARTITIONED BY (kenmei String)\n",
    "STORED AS PARQUET\n",
    "TBLPROPERTIES (\"parquet.compression\"=\"SNAPPY\");\n",
    "LOCATION '/home/pyspark/super_crush_course.db/parquet_table';\n",
    "\n",
    "```\n",
    "\n",
    "圧縮形式と保存するファイルフォーマットを指定してテーブルの作成を行なっていきます。  \n",
    "ちなみにテーブル定義のカラムは、今回読み込んだCSV/JSONのスキーマになります。  \n",
    "Partitionとはデータの区切りのことで、kenmeiを区切りとしてデータを保存しています(次のレクチャーにて)。　 \n",
    "\n",
    "```\n",
    "# パーティションなしのテーブルの場合\n",
    "CREATE TABLE IF NOT EXISTS super_crush_course.parquet_table_with_no_partition\n",
    "(code String, gengo String,kenmei, Stringwareki String,seireki String,chu String,sokei String,jinko_male String,jinko_femail String)\n",
    "STORED AS PARQUET\n",
    "TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "LOCATION '/home/pyspark/super_crush_course.db/parquet_table_with_no_partition'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## テーブル作成\n",
    "# テーブル作成も同様に、spark.sqlを使ってテーブルを作成していきます。\n",
    "# ロケーションはセクション2で出力したディレクトリになります。\n",
    "\n",
    "# Parquetテーブルの作成(パーティションなし)\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS super_crush_course.parquet_table_with_partition \n",
    "(code String, gengo String,wareki String,seireki String,chu String,sokei String,jinko_male String,jinko_femail String)\n",
    "PARTITIONED BY (kenmei String)\n",
    "STORED AS PARQUET\n",
    "TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "LOCATION '/home/pyspark/super_crush_course.db/parquet_table_with_partition'\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# Paruqetテーブルの作成(パーティションあり)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パーティションなしのテーブルも作成します\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS super_crush_course.parquet_table_with_no_partition\n",
    "(code String, gengo String,kenmei, Stringwareki String,seireki String,chu String,sokei String,jinko_male String,jinko_femail String)\n",
    "STORED AS PARQUET\n",
    "TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "LOCATION '/home/pyspark/super_crush_course.db/parquet_table_with_no_partition'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD PRTITION\n",
    "パーティションを認識するためにコマンドを発行する必要があります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パーティションを追加していない場合にクエリしてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パーティションを追加してみる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再度検索を行ってみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create View\n",
    "ビューを作成します。\n",
    "ビューとは、仮想的なテーブルのことでデータを生成しなくてもテーブルを生成することが可能です。\n",
    "\n",
    "言葉で伝えるより実際に見た方がいいと思うので、早速作ってみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create viewを実演を行う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テンポラリテーブル\n",
    "テンポラリーテーブルとは一時的にテーブルを作成することで、SparkSessionごとに生成が可能です。\n",
    "\n",
    "特にスキーマオンリードで読み込んだdataframeを一時的にテーブルにすることで、SQLでの操作を可能にすることができます。\n",
    "\n",
    "一連の流れを見てみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テンポラリーテーブルの実演"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ビッグデータ世界のDMLとは\n",
    "\n",
    "ビッグデータの世界のSQLは基本的にSQL’ライク’です。  \n",
    "というのもRDSのSQLを前提にしてライクと言っているだけなので、ビッグデータ世界を中心としたらSQLそのものです。  \n",
    "RDSでのSQLに慣れている人は、ビッグデータの世界のSQLは難なくこなすことができると思います。  \n",
    "\n",
    "- SELECT\n",
    "- CTAS\n",
    "- SELECT INSERT\n",
    "- INSERT?\n",
    "- UPDATE?\n",
    "- DELETE?\n",
    "\n",
    "今回は4つのDMLを確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT\n",
    "spark.sql(\"select * from parquet_table_with_no_partition\")\n",
    "\n",
    "## パーティションつきのテーブルを\n",
    "spark.sql(\"select * from parquet_table_with_partition where kenmei ='東京都'\")\n",
    "\n",
    "\n",
    "# パーティションなしの場合は、すべてのデータを走査してから絞り込みます\n",
    "# パーティションありの場合は、特定のパーティション配下のディレクトリのみをチェックします"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTAS\n",
    "Create Table As Selectの略です。\n",
    "\n",
    "簡単にいうと、Selectの返却結果からテーブルを作成することが可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTASの実演"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECT INSERT\n",
    "SELECT INSERTの場合は、ADD PARTITIONは不要です"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT INSERTの実演"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSERT/UPDATE/DELETE?\n",
    "ビッグデータの世界では原則としてACIDをサポートしていません。  \n",
    "そのため、UPDATEやDELETがサポートされていないことが多いです。\n",
    "\n",
    "INSERTは単体で利用することはできますが、あまり出番がなく前述で紹介したSELECT INSERTでの出番が大半です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分析関数の練習をしよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ピボットテーブル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAG関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "578f5f657c2fb65ecadb997ad60e5cf2da380ecec34305a6dd913dc5b96e257c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
