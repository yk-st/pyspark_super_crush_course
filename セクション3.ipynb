{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本セクションの目次\n",
    "1. Avroフォーマット\n",
    "2. 前方互換と後方互換と完全互換\n",
    "3. メッセージキューとAvroを連携してみよう\n",
    "4. Avroファイルの読み書き\n",
    "5. Avroで前方互換をやってみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンソールで設定したSparkとNoteBookを接続します(動かす前に毎度実行する必要があります)\n",
    "import findspark\n",
    "findspark.init(\"/home/pyspark/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-streaming_2.13:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-avro_2.12:3.2.0\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# パッケージを複数渡したい時は「,」で繋いで渡します。\n",
    "# Sparkのバージョンにしっかりと合わせます(今回はSparkのバージョンが3.2を使っています。)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ビッグデータの世界のDDL\n",
    "\n",
    "ビッグデータの世界でのDDLはRDSと同じ様にDDL文を実行することが可能です。  \n",
    "今回は以下のDDLについてみていきましょう  \n",
    "\n",
    "- Create Database 文\n",
    "- CREATE EXTERNAL TABLE文\n",
    "- CREATE VIEW\n",
    "- ADD PARTITION(MSCK REPAIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE DATABASE文\n",
    "データベースの作成を行います。\n",
    "こちらはRDSなどのCreate Database　と同じ方法で作成が可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"create database if not exists super_crush_course\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データベースの一覧を見てみましょう\n",
    "spark.sql(\"show databases\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE EXTERNAL TABLE文\n",
    "テーブル定義の構成要素をみていきましょう\n",
    "\n",
    "```\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS super_crush_course.csv_table ( id INT, date STRING)\n",
    "PARTITIONED BY (dt INT)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LOCATION '/home/pyspark/super_crush_course.db/csv_table/dataset/parquet/';\n",
    "\n",
    "#S3などであれば、以下のように設定を変えることも可能です。\n",
    "LOCATION 's3://data.platform/super_crush_course.db/raw_zone/sampletable/';\n",
    "\n",
    "```\n",
    "\n",
    "ビッグデータの世界では、実データとデータベース定義/テーブル定義(メタデータ)は明確に分離されています。  \n",
    "今回のコースだと実データはローカル端末のSSD(HDD)でテーブル定義はMysqlに登録されています。  \n",
    "明確に分離されているからこそ、場所を指し示す宣言であるLocationが必要になってきます  \n",
    "\n",
    "ロケーションは「super_crush_course.db」とDB名.db/TABLE名とすることが通例です。  \n",
    "Externalは外部のという意味で、オンプレ環境の場合はつけないことが多かったが、クラウド環境ではつけるのが必須となっている設定。\n",
    "\n",
    "メタデータについてさらに詳しく知りたい方は、以下の記事を参照してみてください  \n",
    "「【PythonとSparkで始めるデータマネジメント入門】 ビッグデータレイクのための統合メタデータ管理入門」\n",
    "\n",
    "上記の例は、CSV形式のテーブルです。  \n",
    "それ以外にもParquet形式、Avro形式でテーブルを作成することができます。\n",
    "\n",
    "```\n",
    "# パーティションつきのテーブル\n",
    "CREATE TABLE IF NOT EXISTS super_crush_course.parquet_table \n",
    "(code String, gengo String,wareki String,seireki String,chu String,sokei String,jinko_male String,jinko_femail String)\n",
    "PARTITIONED BY (kenmei String)\n",
    "STORED AS PARQUET\n",
    "TBLPROPERTIES (\"parquet.compression\"=\"SNAPPY\");\n",
    "LOCATION '/home/pyspark/super_crush_course.db/parquet_table';\n",
    "\n",
    "```\n",
    "\n",
    "圧縮形式と保存するファイルフォーマットを指定してテーブルの作成を行なっていきます。  \n",
    "ちなみにテーブル定義のカラムは、今回読み込んだCSV/JSONのスキーマになります。  \n",
    "Partitionとはデータの区切りのことで、kenmeiを区切りとしてデータを保存しています(次のレクチャーにて)。　 \n",
    "\n",
    "```\n",
    "# パーティションなしのテーブルの場合\n",
    "CREATE TABLE IF NOT EXISTS super_crush_course.parquet_table_with_no_partition\n",
    "(code String, gengo String,kenmei, Stringwareki String,seireki String,chu String,sokei String,jinko_male String,jinko_femail String)\n",
    "STORED AS PARQUET\n",
    "TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "LOCATION '/home/pyspark/super_crush_course.db/parquet_table_with_no_partition'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## テーブル作成\n",
    "# テーブル作成も同様に、spark.sqlを使ってテーブルを作成していきます。\n",
    "# ロケーションはセクション2で出力したディレクトリになります。\n",
    "\n",
    "# Parquetテーブルの作成(パーティションあり)\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS super_crush_course.parquet_table_with_partition \n",
    "(code String, gengo String,wareki String,seireki String,chu String,sokei String,jinko_male String,jinko_femail String)\n",
    "PARTITIONED BY (kenmei String)\n",
    "STORED AS PARQUET\n",
    "TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "LOCATION '/home/pyspark/super_crush_course.db/parquet_table_with_partition'\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# Paruqetテーブルの作成(パーティションなし)\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS super_crush_course.parquet_table_with_no_partition \n",
    "(code String, gengo String,wareki String,kenmei String,seireki String,chu String,sokei String,jinko_male String,jinko_femail String)\n",
    "STORED AS PARQUET\n",
    "TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "LOCATION '/home/pyspark/super_crush_course.db/parquet_table_with_no_partition'\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVROテーブルの作成(パーティションあり)\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS super_crush_course.avro_table_with_partition \n",
    "(code String, gengo String,wareki String,seireki String,chu String,sokei String,jinko_male String,jinko_femail String)\n",
    "PARTITIONED BY (kenmei String)\n",
    "STORED AS AVRO\n",
    "TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "LOCATION '/home/pyspark/super_crush_course.db/parquet_table_with_partition'\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# AVROテーブルの作成(パーティションなし)\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS super_crush_course.avro_table_with_no_partition \n",
    "(code String, gengo String,wareki String,kenmei String,seireki String,chu String,sokei String,jinko_male String,jinko_femail String)\n",
    "STORED AS AVRO\n",
    "TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "LOCATION '/home/pyspark/super_crush_course.db/avro_table_with_no_partition'\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作成したテーブルを見てみましょう\n",
    "spark.sql(\"show tables in super_crush_course\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD PRTITION\n",
    "パーティションを認識するためにコマンドを発行する必要があります。\n",
    "\n",
    "- Add Partiton\n",
    "- MSCK repair Table\n",
    "\n",
    "の2つをみていきましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パーティションも管理されている\n",
    "spark.sql(\"show partitions super_crush_course.parquet_table_with_partition\").show(n=2)\n",
    "# 本来件名があってほしいが。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パーティションがテーブルは。。？クエリしてみる\n",
    "spark.sql(\"select * from super_crush_course.parquet_table_with_no_partition\").show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パーティションがあってadd paritionをしていないテーブルは。。？クエリしてみる\n",
    "spark.sql(\"select * from super_crush_course.parquet_table_with_partition\").show(n=2)\n",
    "# データを見ることができない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パーティションを追加してみる\n",
    "# パーティションの追加には２種類存在しています\n",
    "# add partition\n",
    "# msck repair table名\n",
    "spark.sql(\"alter table super_crush_course.parquet_table_with_partition add if not exists  partition (kenmei='東京都')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パーティションも管理されている\n",
    "spark.sql(\"show partitions super_crush_course.parquet_table_with_partition\").show(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再度検索を行ってみる\n",
    "# パーティションがあってadd paritionをしていないテーブルは。。？クエリしてみる\n",
    "spark.sql(\"select * from super_crush_course.parquet_table_with_partition\").show()\n",
    "# 追加した東京都のデータだけ見える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一個づつAdd partitionするのは面倒なのでmsckを使う(ただし時間がかかる場合が多いので、日々の処理であればadd partitionを選択する方がいい)\n",
    "spark.sql(\"msck repair table super_crush_course.parquet_table_with_partition\")\n",
    "spark.sql(\"msck repair table super_crush_course.avro_table_with_partition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create View\n",
    "ビューを作成します。\n",
    "ビューとは、仮想的なテーブルのことでデータを生成しなくてもテーブルを生成することが可能です。\n",
    "\n",
    "言葉で伝えるより実際に見た方がいいと思うので、早速作ってみましょう。　　\n",
    "\n",
    "手っ取り早くクエリを簡単にしたい場合に有効です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create view\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "create view parquet_view (gengo)\n",
    "as \n",
    "select gengo from \n",
    "super_crush_course.parquet_table_with_partition\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from parquet_view\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テンポラリテーブル\n",
    "テンポラリーテーブルとはデータフレームから一時的にテーブルを作成することで、SparkSessionごとに生成が可能です。\n",
    "\n",
    "特にスキーマオンリードで読み込んだdataframeを一時的にテーブルにすることで、SQLでの操作を可能にすることができます。\n",
    "\n",
    "一連の流れを見てみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テンポラリーテーブルの作成\n",
    "json_df=spark.read.json(\"./dataset/jinko.json\")\n",
    "json_df.createOrReplaceTempView(\"json_tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　json_tempの検索\n",
    "spark.sql(\"select * from json_tmp where kenmei='東京都'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ビッグデータ世界のDMLとは\n",
    "\n",
    "ビッグデータの世界のSQLは基本的にSQL’ライク’です。  \n",
    "というのもRDSのSQLを前提にしてライクと言っているだけなので、ビッグデータ世界を中心としたらSQLそのものです。  \n",
    "RDSでのSQLに慣れている人は、ビッグデータの世界のSQLは難なくこなすことができると思います。  \n",
    "\n",
    "- SELECT\n",
    "- CTAS\n",
    "- SELECT INSERT\n",
    "- INSERT?\n",
    "- UPDATE?\n",
    "- DELETE?\n",
    "\n",
    "今回は4つのDMLを確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT\n",
    "spark.sql(\"select * from super_crush_course.parquet_table_with_no_partition\").show()\n",
    "\n",
    "## パーティションつきのテーブルを検索してみる\n",
    "spark.sql(\"select * from super_crush_course.parquet_table_with_partition where kenmei ='東京都'\").show()\n",
    "\n",
    "# パーティションなしの場合は、すべてのデータを走査してから絞り込みます\n",
    "# パーティションありの場合は、特定のパーティション配下のディレクトリのみをチェックします\n",
    "\n",
    "# 大体のRDSのSQLでできることは実行可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTAS\n",
    "Create Table As Selectの略です。\n",
    "\n",
    "簡単にいうと、Selectの返却結果からテーブルを作成することが可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CTASを動かしてみます\n",
    "\n",
    "# SQLでやる方法\n",
    "spark.sql(\"\"\" \n",
    "CREATE EXTERNAL TABLE if not exists super_crush_course.ctas_sql \n",
    "    STORED AS PARQUET LOCATION '/home/pyspark/super_crush_course.db/ctas_sql' \n",
    "AS\n",
    "SELECT *\n",
    "    FROM super_crush_course.parquet_table_with_no_partition\n",
    "WHERE 1=1\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------------------+-----------+\n",
      "|namespace         |tableName                      |isTemporary|\n",
      "+------------------+-------------------------------+-----------+\n",
      "|super_crush_course|avro_table_with_no_partition   |false      |\n",
      "|super_crush_course|avro_table_with_partition      |false      |\n",
      "|super_crush_course|ctas_dataframe                 |false      |\n",
      "|super_crush_course|ctas_sql                       |false      |\n",
      "|super_crush_course|parquet_table_with_no_partition|false      |\n",
      "|super_crush_course|parquet_table_with_partition   |false      |\n",
      "|                  |json_tmp                       |true       |\n",
      "+------------------+-------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables in super_crush_course\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/06 03:11:03 WARN FileUtils: File file:/home/pyspark/pyspark_super_crush_course/spark-warehouse/super_crush_course.db/ctas_dataframe does not exist; Force to delete it.\n",
      "22/01/06 03:11:03 ERROR FileUtils: Failed to delete file:/home/pyspark/pyspark_super_crush_course/spark-warehouse/super_crush_course.db/ctas_dataframe\n"
     ]
    }
   ],
   "source": [
    "json_df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"super_crush_course.ctas_dataframe\",path='/home/pyspark/super_crush_course.db/ctas_dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- chu: string (nullable = true)\n",
      " |-- code: string (nullable = true)\n",
      " |-- gengo: string (nullable = true)\n",
      " |-- jinko_female: string (nullable = true)\n",
      " |-- jinko_male: string (nullable = true)\n",
      " |-- kenmei: string (nullable = true)\n",
      " |-- seireki: string (nullable = true)\n",
      " |-- sokei: string (nullable = true)\n",
      " |-- wareki: string (nullable = true)\n",
      "\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                                                                                                                                                                                   |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE TABLE `super_crush_course`.`ctas_dataframe` (\\n  `chu` STRING,\\n  `code` STRING,\\n  `gengo` STRING,\\n  `jinko_female` STRING,\\n  `jinko_male` STRING,\\n  `kenmei` STRING,\\n  `seireki` STRING,\\n  `sokei` STRING,\\n  `wareki` STRING)\\nUSING parquet\\nLOCATION 'file:/home/pyspark/super_crush_course.db/ctas_dataframe'\\n|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from super_crush_course.ctas_dataframe\").printSchema()\n",
    "spark.sql(\"show create table super_crush_course.ctas_dataframe\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------------+----------+--------+-------+--------+------+\n",
      "| chu|code|gengo|jinko_female|jinko_male|  kenmei|seireki|   sokei|wareki|\n",
      "+----+----+-----+------------+----------+--------+-------+--------+------+\n",
      "|null|  00| 大正|    27918868|  28044185|    全国|   1920|55963053|     9|\n",
      "|null|  01| 大正|     1114861|   1244322|  北海道|   1920| 2359183|     9|\n",
      "|null|  02| 大正|      375161|    381293|  青森県|   1920|  756454|     9|\n",
      "|null|  03| 大正|      424471|    421069|  岩手県|   1920|  845540|     9|\n",
      "|null|  04| 大正|      476459|    485309|  宮城県|   1920|  961768|     9|\n",
      "|null|  05| 大正|      444855|    453682|  秋田県|   1920|  898537|     9|\n",
      "|null|  06| 大正|      490597|    478328|  山形県|   1920|  968925|     9|\n",
      "|null|  07| 大正|      689225|    673525|  福島県|   1920| 1362750|     9|\n",
      "|null|  08| 大正|      688272|    662128|  茨城県|   1920| 1350400|     9|\n",
      "|null|  09| 大正|      532224|    514255|  栃木県|   1920| 1046479|     9|\n",
      "|null|  10| 大正|      538504|    514106|  群馬県|   1920| 1052610|     9|\n",
      "|null|  11| 大正|      678372|    641161|  埼玉県|   1920| 1319533|     9|\n",
      "|null|  12| 大正|      679187|    656968|  千葉県|   1920| 1336155|     9|\n",
      "|null|  13| 大正|     1746439|   1952989|  東京都|   1920| 3699428|     9|\n",
      "|null|  14| 大正|      633639|    689751|神奈川県|   1920| 1323390|     9|\n",
      "|null|  15| 大正|      904942|    871532|  新潟県|   1920| 1776474|     9|\n",
      "|null|  16| 大正|      369501|    354775|  富山県|   1920|  724276|     9|\n",
      "|null|  17| 大正|      382985|    364375|  石川県|   1920|  747360|     9|\n",
      "|null|  18| 大正|      305974|    293181|  福井県|   1920|  599155|     9|\n",
      "|null|  19| 大正|      292636|    290817|  山梨県|   1920|  583453|     9|\n",
      "+----+----+-----+------------+----------+--------+-------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from super_crush_course.ctas_dataframe\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECT INSERT\n",
    "既に存在するテーブルに対して、検索結果をもとにデータを登録していくことができます。　　\n",
    "SELECT INSERTの場合は、ADD PARTITIONは不要です\n",
    "\n",
    "CTASと違うのはこちらはテーブルを作る操作ではなくて既にあるテーブルに対してデータを登録することが目的です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/06 03:07:41 WARN log: Updating partition stats fast for: parquet_table_with_partition\n",
      "22/01/06 03:07:41 WARN log: Updated size to 22841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 今回は各地域のデータの履歴をまとめて一つのパーティションに入れるSelect Insertを記載してましょう。\n",
    "spark.sql(\"\"\" \n",
    "Insert overwrite table super_crush_course.parquet_table_with_partition PARTITION(kenmei='all')\n",
    "\n",
    "select code,gengo,wareki,seireki,chu,sokei,jinko_male,jinko_femail\n",
    " from \n",
    "super_crush_course.parquet_table_with_no_partition\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|                    partition|\n",
      "+-----------------------------+\n",
      "|         kenmei=__HIVE_DEF...|\n",
      "|                   kenmei=all|\n",
      "|                kenmei=三重県|\n",
      "|                kenmei=京都府|\n",
      "|          kenmei=人口集中地区|\n",
      "|kenmei=人口集中地区以外の地区|\n",
      "|                kenmei=佐賀県|\n",
      "|                  kenmei=全国|\n",
      "|                kenmei=兵庫県|\n",
      "|                kenmei=北海道|\n",
      "|                kenmei=千葉県|\n",
      "|              kenmei=和歌山県|\n",
      "|                kenmei=埼玉県|\n",
      "|                kenmei=大分県|\n",
      "|                kenmei=大阪府|\n",
      "|                kenmei=奈良県|\n",
      "|                kenmei=宮城県|\n",
      "|                kenmei=宮崎県|\n",
      "|                kenmei=富山県|\n",
      "|                kenmei=山口県|\n",
      "+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show partitions super_crush_course.parquet_table_with_partition\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSERT/UPDATE/DELETE?\n",
    "ビッグデータの世界では原則としてACIDをサポートしていません。  \n",
    "そのため、UPDATEやDELETがサポートされていないことが多いです。\n",
    "\n",
    "INSERTは単体で利用することはできますが、あまり出番がなく前述で紹介したSELECT INSERTでの出番が大半です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分析関数の練習をしよう\n",
    "ここからは、分析関数の練習をしてみましょう。  \n",
    "\n",
    "- agg(groupby,count,sum)\n",
    "- window(over)\n",
    "- ピボットテーブル\n",
    "- lag関数\n",
    "\n",
    "データフレームでの操作とSQLでの操作を対比させながら実行していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agg\n",
    "# データを溜め込んでいくこと"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ピボットテーブル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAG関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データフレーム限定 RDDによる一行づつの操作\n",
    "\n",
    "出番がある様な、無い様な操作ですが一つSparkの特徴であるRDD(低レベル操作)についてみていきましょう。  \n",
    "Sparkは全てのDataFrameは実行されるときにRDDに変換されて実行されます(そのときに最適なRDD操作に変換してくれる)。\n",
    "\n",
    "あまり普段RDDを意識することなく操作を可能です。\n",
    "\n",
    "一応RDDでの操作も見ておきましょう。\n",
    "\n",
    "RDDに変換すると、mapとかlambdaなどPythonの関数が適用になりますが、最適化をやってくれなくなるのであまりおすすめはできないです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdds=json_df.rdd.map(lambda x: len(x.code))\n",
    "rdds.take(10)\n",
    "\n",
    "rdds.reduce(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "578f5f657c2fb65ecadb997ad60e5cf2da380ecec34305a6dd913dc5b96e257c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
