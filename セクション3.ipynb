{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本セクションの目次\n",
    "1. Avroフォーマット\n",
    "2. 前方互換と後方互換と完全互換\n",
    "3. メッセージキューとAvroを連携してみよう\n",
    "4. Avroファイルの読み書き\n",
    "5. Avroで前方互換をやってみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# カラムナーフォーマット\n",
    "ここまでで読み込んだJsonやCsvはローデータとよばれる、データ基盤では扱いづらい部類に入るファイルです。  \n",
    "扱いづらい理由はファイルがスプリッタブルかどうか？という点が関わっています。  \n",
    "\n",
    "スプリッタブルであればデータを複数端末で分割して処理することが可能ですし、そうでなければ分割して処理をすることができないため非効率が発生します。  \n",
    "\n",
    "カラムナーフォーマットと次のレクチャーで紹介するAvro(行指向フォーマット)はいずれもスプリッタブルなファイルです。  \n",
    "そのため、CSVやJSON形式のままデータ基盤で活用を続けるのは悪手なので、必ずカラムナーフォーマット、行指向フォーマットへ変換する処理を挟みましょう。  \n",
    "\n",
    "カラムナーフォーマットとは、分析に特化したファイル形式です。  \n",
    "列ごとにデータをまとめて保存することによって、データを圧縮し検索速度(特にGroup byなどの集計構文)をあげています。  \n",
    "また、カラムナーフォーマットは内部にカラム情報を保持しているためセクション2で紹介したスキーマオンリード機能を使ってスキーマ定義を読み出すことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 圧縮形式とファイルフォーマット\n",
    "\n",
    "前のレクチャーではスプリッタブルかどうかが重要であるとお伝えしました。  \n",
    "ここで一度、ファイルと圧縮形式の組み合わせについてみていきましょう。\n",
    "\n",
    "```\n",
    "        paruqet avro csv/json\n",
    "gz       Y      Y      N\n",
    "snappy   Y      Y      -\n",
    "bz2      -      -      Y\n",
    "圧縮なし  Y      Y      N\n",
    "\n",
    "```\n",
    "\n",
    "大事なのは、csvやjsonの場合はbz2を利用しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 行指向フォーマット\n",
    "先ほどはカラムナーフォーマットを紹介しましたが、次は行指向フォーマットを紹介について紹介していきます。  \n",
    "\n",
    "よく利用されているRDBも行指向の仕組みを持ったデータシステムです。  \n",
    "RDBはレコードの追加が得意です。  \n",
    " \n",
    "そして、ビッグデータはレコードを一件つづ追加していく様な処理が苦手でしたが行指向フォーマットのAvroの登場によってレコードの追加が頻繁に発生する処理  \n",
    "つまりストリーミング処理にも適用することが可能になりました。  \n",
    "\n",
    "まず覚えてほしいのは、Avroはメインとしてストリーミング処理の利用に向いているということです（バッチ処理にも使うことが可能です）  \n",
    "\n",
    "\n",
    "ビッグデータにおけるストリーミングについて詳しく知りたい方は  \n",
    "「データサイエンスのためのストリーミング前処理入門　PythonとSparkで始めるビッグデータストリーミング処理入門」\n",
    "を是非受講ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンソールで設定したSparkとNoteBookを接続します(動かす前に毎度実行する必要があります)\n",
    "import findspark\n",
    "findspark.init(\"/home/pyspark/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-streaming_2.13:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-avro_2.12:3.2.0\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# パッケージを複数渡したい時は「,」で繋いで渡します。\n",
    "# Sparkのバージョンにしっかりと合わせます(今回はSparkのバージョンが3.2を使っています。)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframeとは？\n",
    "\n",
    "データを枠組み(フレーム)として扱うことができるのがDataFrameです。  \n",
    "データフレームの作成方法は\n",
    "\n",
    "- SQLから生成する\n",
    "- データを読み込んで生成する\n",
    "- プログラム的に作成する\n",
    "\n",
    "SQLから生成はSQLで取得した結果がデータフレームで返却されるます  \n",
    "\n",
    "データを読み込んで生成する場合はCSVやJSon、Parquet、Avroなどのファイルを読み込んでデータフレームに取り込みを行うことが可能  \n",
    "\n",
    "プログラム的に作成するのは、空のデータフレームが欲しい時など"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ビッグデータの世界のDDL\n",
    "\n",
    "ビッグデータの世界でのDDLはRDSと同じ様にDDL文を実行することが可能です。  \n",
    "今回は以下のDDLについてみていきましょう  \n",
    "\n",
    "- Create Database 文\n",
    "- CREATE EXTERNAL TABLE文\n",
    "- CREATE VIEW\n",
    "- ADD PARTITION(MSCK REPAIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE DATABASE文\n",
    "データベースの作成を行います。\n",
    "こちらはRDSなどのCreate Database　と同じ方法で作成が可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"create database if not exists super_crush_course\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE EXTERNAL TABLE文\n",
    "テーブル定義の構成要素をみていきましょう\n",
    "\n",
    "```\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS super_crush_course.csv_table ( id INT, date STRING)\n",
    "PARTITIONED BY (dt INT)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LOCATION '/home/pyspark/super_crush_course.db/csv_table/dataset/parquet/';\n",
    "\n",
    "#S3などであれば、以下のように設定を変えることも可能です。\n",
    "LOCATION 's3://data.platform/super_crush_course.db/raw_zone/sampletable/';\n",
    "\n",
    "```\n",
    "\n",
    "ビッグデータの世界では、実データとデータベース定義/テーブル定義(メタデータ)は明確に分離されています。  \n",
    "今回のコースだと実データはローカル端末のSSD(HDD)でテーブル定義はMysqlに登録されています。  \n",
    "明確に分離されているからこそ、場所を指し示す宣言であるLocationが必要になってきます  \n",
    "\n",
    "ロケーションは「super_crush_course.db」とDB名.db/TABLE名とすることが通例です。  \n",
    "Externalは外部のという意味で、オンプレ環境の場合はつけないことが多かったが、クラウド環境ではつけるのが必須となっている設定。\n",
    "\n",
    "メタデータについてさらに詳しく知りたい方は、以下の記事を参照してみてください  \n",
    "「【PythonとSparkで始めるデータマネジメント入門】 ビッグデータレイクのための統合メタデータ管理入門」\n",
    "\n",
    "上記の例は、CSV形式のテーブルです。  \n",
    "それ以外にもParquet形式、Avro形式でテーブルを作成することができます。\n",
    "\n",
    "```\n",
    "# メタデータ保存用のSparkテーブル\n",
    "CREATE TABLE IF NOT EXISTS super_crush_course.parquet_table \n",
    "(code String, gengo String,wareki String,seireki String,chu String,sokei String,jinko_male String,jinko_femail String)\n",
    "PARTITIONED BY (kenmei String)\n",
    "STORED AS PARQUET\n",
    "TBLPROPERTIES (\"parquet.compression\"=\"SNAPPY\");\n",
    "LOCATION '/home/pyspark/super_crush_course.db/parquet_table';\n",
    "\n",
    "```\n",
    "\n",
    "圧縮形式と保存するファイルフォーマットを指定してテーブルの作成を行なっていきます。  \n",
    "ちなみにテーブル定義のカラムは、今回読み込んだCSV/JSONのスキーマになります。  \n",
    "Partitionとはデータの区切りで　 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## テーブル作成\n",
    "# テーブル作成も同様に、spark.sqlを使ってテーブルを作成していきます。\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS super_crush_course.parquet_table \n",
    "(code String, gengo String,wareki String,seireki String,chu String,sokei String,jinko_male String,jinko_femail String)\n",
    "PARTITIONED BY (kenmei String)\n",
    "STORED AS PARQUET\n",
    "TBLPROPERTIES (\"parquet.compression\"=\"SNAPPY\");\n",
    "LOCATION '/home/pyspark/super_crush_course.db/parquet_table';\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create View\n",
    "ビューを作成します。\n",
    "ビューとは、仮想的なテーブルのことでデータを生成しなくてもテーブルを生成することが可能です。\n",
    "\n",
    "言葉で伝えるより実際に見た方がいいと思うので、早速作ってみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create viewを実演を行う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テンポラリテーブル\n",
    "テンポラリーテーブルとは一時的にテーブルを作成することで、SparkSessionごとに生成が可能です。\n",
    "\n",
    "特にスキーマオンリードで読み込んだdataframeを一時的にテーブルにすることで、SQLでの操作を可能にすることができます。\n",
    "\n",
    "一連の流れを見てみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テンポラリーテーブルの実演"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ビッグデータ世界のDMLとは\n",
    "\n",
    "ビッグデータの世界のSQLは基本的にSQL’ライク’です。  \n",
    "というのもRDSのSQLを前提にしてライクと言っているだけなので、ビッグデータ世界を中心としたらSQLそのものです。  \n",
    "RDSでのSQLに慣れている人は、ビッグデータの世界のSQLは難なくこなすことができると思います。  \n",
    "\n",
    "- SELECT\n",
    "- INSERT?\n",
    "- UPDATE?\n",
    "- DELETE?\n",
    "- CTAS\n",
    "- SELECT INSERT\n",
    "\n",
    "今回は4つのDMLを確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"show tables in super_crush_course\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分析関数の練習をしよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ピボットテーブル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAG関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "578f5f657c2fb65ecadb997ad60e5cf2da380ecec34305a6dd913dc5b96e257c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
