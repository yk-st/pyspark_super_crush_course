{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本セクションの目次\n",
    "1. ビッグデータ世界のDDL\n",
    "2. テンポラリテーブル\n",
    "3. ビッグデータ世界のSQLとは？\n",
    "4. 単純SQLを振り返ってみよう\n",
    "5. 分析関数を練習してみよう\n",
    "6. LAG/Lead関数\n",
    "7. ピボットテーブル\n",
    "8. SparkのRDDを使って1レコードつづ処理してみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンソールで設定したSparkとNoteBookを接続します(動かす前に毎度実行する必要があります)\n",
    "import findspark\n",
    "findspark.init(\"/home/pyspark/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-streaming_2.13:3.2.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1,org.apache.spark:spark-avro_2.12:3.2.1\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# パッケージを複数渡したい時は「,」で繋いで渡します。\n",
    "# Sparkのバージョンにしっかりと合わせます(今回はSparkのバージョンが3.2を使っています。)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ビッグデータの世界のDDL\n",
    "\n",
    "ビッグデータの世界でのDDLはRDSと同じ様にDDL文を実行することが可能です。  \n",
    "今回は以下のDDLについてみていきましょう  \n",
    "\n",
    "- Create Database 文\n",
    "- CREATE EXTERNAL TABLE文\n",
    "- CREATE VIEW\n",
    "- ADD PARTITION(MSCK REPAIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE DATABASE文\n",
    "データベースの作成を行います。\n",
    "こちらはRDSなどのCreate Database　と同じ方法で作成が可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"create database if not exists super_crush_course\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データベースの一覧を見てみましょう\n",
    "spark.sql(\"show databases\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE EXTERNAL TABLE文\n",
    "テーブル定義の構成要素をみていきましょう\n",
    "\n",
    "```\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS super_crush_course.csv_table ( id INT, date STRING)\n",
    "PARTITIONED BY (dt INT)\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "LOCATION '/home/pyspark/super_crush_course.db/csv_table/dataset/parquet/';\n",
    "\n",
    "#S3などであれば、以下のように設定を変えることも可能です。\n",
    "LOCATION 's3://data.platform/super_crush_course.db/raw_zone/sampletable/';\n",
    "\n",
    "```\n",
    "\n",
    "ビッグデータの世界では、実データとデータベース定義/テーブル定義(メタデータ)は明確に分離されています。  \n",
    "今回のコースだと実データはローカル端末のSSD(HDD)でテーブル定義はMysqlに登録されています。  \n",
    "明確に分離されているからこそ、場所を指し示す宣言であるLocationが必要になってきます  \n",
    "\n",
    "ロケーションは「super_crush_course.db」とDB名.db/TABLE名とすることが通例です。  \n",
    "Externalは外部のという意味で、オンプレ環境の場合はつけないことが多かったが、クラウド環境ではつけるのが必須となっている設定。\n",
    "\n",
    "メタデータについてさらに詳しく知りたい方は、以下の記事を参照してみてください  \n",
    "「【PythonとSparkで始めるデータマネジメント入門】 ビッグデータレイクのための統合メタデータ管理入門」\n",
    "\n",
    "上記の例は、CSV形式のテーブルです。  \n",
    "それ以外にもParquet形式、Avro形式でテーブルを作成することができます。\n",
    "\n",
    "```\n",
    "# パーティションつきのテーブル\n",
    "CREATE TABLE IF NOT EXISTS super_crush_course.parquet_table \n",
    "(code String, gengo String,wareki String,seireki String,chu String,sokei String,jinko_male String,jinko_femail String)\n",
    "PARTITIONED BY (kenmei String)\n",
    "STORED AS PARQUET\n",
    "TBLPROPERTIES (\"parquet.compression\"=\"SNAPPY\");\n",
    "LOCATION '/home/pyspark/super_crush_course.db/parquet_table';\n",
    "\n",
    "```\n",
    "\n",
    "圧縮形式と保存するファイルフォーマットを指定してテーブルの作成を行なっていきます。  \n",
    "ちなみにテーブル定義のカラムは、今回読み込んだCSV/JSONのスキーマになります。  \n",
    "Partitionとはデータの区切りのことで、kenmeiを区切りとしてデータを保存しています(次のレクチャーにて)。　 \n",
    "\n",
    "```\n",
    "# パーティションなしのテーブルの場合\n",
    "CREATE TABLE IF NOT EXISTS super_crush_course.parquet_table_with_no_partition\n",
    "(code String, gengo String,kenmei, Stringwareki String,seireki String,chu String,sokei String,jinko_male String,jinko_femail String)\n",
    "STORED AS PARQUET\n",
    "TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "LOCATION '/home/pyspark/super_crush_course.db/parquet_table_with_no_partition'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## テーブル作成\n",
    "# テーブル作成も同様に、spark.sqlを使ってテーブルを作成していきます。\n",
    "# ロケーションはセクション2で出力したディレクトリになります。\n",
    "\n",
    "# Parquetテーブルの作成(パーティションあり)\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS super_crush_course.parquet_table_with_partition \n",
    "(code String, gengo String,wareki String,seireki String,chu String,sokei String,jinko_male String,jinko_femail String)\n",
    "PARTITIONED BY (kenmei String)\n",
    "STORED AS PARQUET\n",
    "TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "LOCATION '/home/pyspark/super_crush_course.db/parquet_table_with_partition'\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# Paruqetテーブルの作成(パーティションなし)\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS super_crush_course.parquet_table_with_no_partition \n",
    "(code String, gengo String,wareki String,kenmei String,seireki String,chu String,sokei String,jinko_male String,jinko_femail String)\n",
    "STORED AS PARQUET\n",
    "TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "LOCATION '/home/pyspark/super_crush_course.db/parquet_table_with_no_partition'\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVROテーブルの作成(パーティションあり)\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS super_crush_course.avro_table_with_partition \n",
    "(code String, gengo String,wareki String,seireki String,chu String,sokei String,jinko_male String,jinko_femail String)\n",
    "PARTITIONED BY (kenmei String)\n",
    "STORED AS AVRO\n",
    "TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "LOCATION '/home/pyspark/super_crush_course.db/parquet_table_with_partition'\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# AVROテーブルの作成(パーティションなし)\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS super_crush_course.avro_table_with_no_partition \n",
    "(code String, gengo String,wareki String,kenmei String,seireki String,chu String,sokei String,jinko_male String,jinko_femail String)\n",
    "STORED AS AVRO\n",
    "TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "LOCATION '/home/pyspark/super_crush_course.db/avro_table_with_no_partition'\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作成したテーブルを見てみましょう\n",
    "spark.sql(\"show tables in super_crush_course\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD PRTITION\n",
    "パーティションを認識するためにコマンドを発行する必要があります。\n",
    "\n",
    "- Add Partiton\n",
    "- MSCK repair Table\n",
    "\n",
    "の2つをみていきましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パーティションも管理されている\n",
    "spark.sql(\"show partitions super_crush_course.parquet_table_with_partition\").show(n=2)\n",
    "# 本来件名があってほしいが。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パーティションがテーブルは。。？クエリしてみる\n",
    "spark.sql(\"select * from super_crush_course.parquet_table_with_no_partition\").show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パーティションがあってadd paritionをしていないテーブルは。。？クエリしてみる\n",
    "spark.sql(\"select * from super_crush_course.parquet_table_with_partition\").show(n=2)\n",
    "# データを見ることができない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パーティションを追加してみる\n",
    "# パーティションの追加には２種類存在しています\n",
    "# add partition\n",
    "# msck repair table名\n",
    "spark.sql(\"alter table super_crush_course.parquet_table_with_partition add if not exists  partition (kenmei='東京都')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パーティションも管理されている\n",
    "spark.sql(\"show partitions super_crush_course.parquet_table_with_partition\").show(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再度検索を行ってみる\n",
    "# パーティションがあってadd paritionをしていないテーブルは。。？クエリしてみる\n",
    "spark.sql(\"select * from super_crush_course.parquet_table_with_partition\").show()\n",
    "# 追加した東京都のデータだけ見える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一個づつAdd partitionするのは面倒なのでmsckを使う(ただし時間がかかる場合が多いので、日々の処理であればadd partitionを選択する方がいい)\n",
    "spark.sql(\"msck repair table super_crush_course.parquet_table_with_partition\")\n",
    "spark.sql(\"msck repair table super_crush_course.avro_table_with_partition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create View\n",
    "ビューを作成します。\n",
    "ビューとは、仮想的なテーブルのことでデータを生成しなくてもテーブルを生成することが可能です。\n",
    "\n",
    "言葉で伝えるより実際に見た方がいいと思うので、早速作ってみましょう。　　\n",
    "\n",
    "手っ取り早くクエリを簡単にしたい場合に有効です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create view\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "create view parquet_view (gengo)\n",
    "as \n",
    "select gengo from \n",
    "super_crush_course.parquet_table_with_partition\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from parquet_view\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テンポラリテーブル\n",
    "テンポラリーテーブルとはデータフレームから一時的にテーブルを作成することで、SparkSessionごとに生成が可能です。\n",
    "\n",
    "特にスキーマオンリードで読み込んだdataframeを一時的にテーブルにすることで、SQLでの操作を可能にすることができます。\n",
    "\n",
    "一連の流れを見てみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テンポラリーテーブルの作成\n",
    "json_df=spark.read.json(\"./dataset/jinko.json\")\n",
    "json_df.createOrReplaceTempView(\"json_tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　json_tempの検索\n",
    "spark.sql(\"select * from json_tmp where kenmei='東京都'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ビッグデータ世界のDMLとは\n",
    "\n",
    "ビッグデータの世界のSQLは基本的にSQL’ライク’です。  \n",
    "というのもRDSのSQLを前提にしてライクと言っているだけなので、ビッグデータ世界を中心としたらSQLそのものです。  \n",
    "RDSでのSQLに慣れている人は、ビッグデータの世界のSQLは難なくこなすことができると思います。  \n",
    "\n",
    "- SELECT\n",
    "- CTAS\n",
    "- SELECT INSERT\n",
    "- INSERT?\n",
    "- UPDATE?\n",
    "- DELETE?\n",
    "\n",
    "今回は4つのDMLを確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT\n",
    "spark.sql(\"select * from super_crush_course.parquet_table_with_no_partition\").show()\n",
    "\n",
    "## パーティションつきのテーブルを検索してみる\n",
    "spark.sql(\"select * from super_crush_course.parquet_table_with_partition where kenmei ='東京都'\").show()\n",
    "\n",
    "# パーティションなしの場合は、すべてのデータを走査してから絞り込みます\n",
    "# パーティションありの場合は、特定のパーティション配下のディレクトリのみをチェックします\n",
    "\n",
    "# 大体のRDSのSQLでできることは実行可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joinも可能\n",
    "# SQLでJoinするというのはRDBを使っている方ならイメージが湧くと思いますので、今回はデータフレームでジョインをしてみたいと思います\n",
    "json_df.join(json_df,on=[json_df.code == json_df.code,json_df.wareki == json_df.wareki],how=\"inner\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTAS\n",
    "Create Table As Selectの略です。\n",
    "\n",
    "簡単にいうと、Selectの返却結果からテーブルを作成することが可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTASを動かしてみます\n",
    "\n",
    "# SQLでやる方法\n",
    "spark.sql(\"\"\" \n",
    "CREATE EXTERNAL TABLE if not exists super_crush_course.ctas_sql \n",
    "    STORED AS PARQUET LOCATION '/home/pyspark/super_crush_course.db/ctas_sql' \n",
    "AS\n",
    "SELECT *\n",
    "    FROM super_crush_course.parquet_table_with_no_partition\n",
    "WHERE 1=1\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables in super_crush_course\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"super_crush_course.ctas_dataframe\",path='/home/pyspark/super_crush_course.db/ctas_dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from super_crush_course.ctas_dataframe\").printSchema()\n",
    "spark.sql(\"show create table super_crush_course.ctas_dataframe\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from super_crush_course.ctas_dataframe\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECT INSERT\n",
    "既に存在するテーブルに対して、検索結果をもとにデータを登録していくことができます。　　\n",
    "SELECT INSERTの場合は、ADD PARTITIONは不要です\n",
    "\n",
    "CTASと違うのはこちらはテーブルを作る操作ではなくて既にあるテーブルに対してデータを登録することが目的です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今回は各地域のデータの履歴をまとめて一つのパーティションに入れるSelect Insertを記載してましょう。\n",
    "spark.sql(\"\"\" \n",
    "Insert overwrite table super_crush_course.parquet_table_with_partition PARTITION(kenmei='all')\n",
    "\n",
    "select code,gengo,wareki,seireki,chu,sokei,jinko_male,jinko_femail\n",
    " from \n",
    "super_crush_course.parquet_table_with_no_partition\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show partitions super_crush_course.parquet_table_with_partition\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSERT/UPDATE/DELETE?\n",
    "ビッグデータの世界では原則としてACIDをサポートしていません。  \n",
    "そのため、UPDATEやDELETがサポートされていないことが多いです。\n",
    "\n",
    "INSERTは単体で利用することはできますが、あまり出番がなく前述で紹介したSELECT INSERTでの出番が大半です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分析関数の練習をしよう\n",
    "ここからは、分析関数の練習をしてみましょう。  \n",
    "\n",
    "- agg(groupby,count,sum)\n",
    "- window(over)\n",
    "- ピボットテーブル\n",
    "- lag関数\n",
    "\n",
    "データフレームでの操作とSQLでの操作を対比させながら実行していきます。\n",
    "\n",
    "# 利用するテーブル(とDataFrame)\n",
    "spark.sql(\"select * from super_crush_course.parquet_table_with_no_partition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df=spark.sql(\"select * from super_crush_course.parquet_table_with_no_partition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F \n",
    "# groupby->agg\n",
    "# groupbyしたそれぞれのグループの中でデータを溜め込んでいく(count,sum,avg)こと\n",
    "\n",
    "analysis_df.groupBy(analysis_df.code).count().show(n=2)\n",
    "\n",
    "analysis_df.groupBy(analysis_df.code).agg(F.sum(analysis_df.jinko_male).alias(\"sum_male\"),F.avg(analysis_df.jinko_male).alias(\"avg_me\")).show(n=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqlで表現してみましょう\n",
    "spark.sql(\"\"\"\n",
    "select code,sum(jinko_male) as sum_male,avg(jinko_male) as avg_me from super_crush_course.parquet_table_with_no_partition\n",
    "group by code\n",
    "\"\"\").show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# window 関数\n",
    "# window->{paritionBy(=GroupBy)->(order by) -> rank(順番付け)}\n",
    "# 最後にwindowをoverにいれると結果が出ます\n",
    "\n",
    "window_schema = Window.partitionBy(analysis_df.code).orderBy(analysis_df.sokei.cast(\"Long\").asc())\n",
    "analysis_df.withColumn(\"sokei_rank\",F.rank().over(window_schema)).show(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLで表現してみましょう\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "*,\n",
    "RANK() OVER (PARTITION BY code ORDER BY cast(sokei as long) asc) as sokei_rank\n",
    "\n",
    "from super_crush_course.parquet_table_with_no_partition \n",
    "\"\"\").show(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAG/LEAD関数\n",
    "Lagというのはひとつ前のレコード\n",
    "LEADというのは一つ先のレコード\n",
    "\n",
    "前日比とか前年比といった様に前後のレコードを比較することによって意味をなす値を出したいときに使います。\n",
    "\n",
    "今回は、先ほど並び替えたデータを使って、どれくらい人口が増えていったのか確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# window 関数\n",
    "# window->{paritionBy(=GroupBy)->(order by) -> rank(順番付け)}\n",
    "# 最後にwindowをoverにいれると結果が出ます\n",
    "\n",
    "window_schema = Window.partitionBy(analysis_df.code).orderBy(analysis_df.sokei.cast(\"Long\").asc())\n",
    "\n",
    "analysis_df.withColumn(\"sokei_per\",F.lag(\"sokei\",1).over(window_schema)).show(n=50)\n",
    "percent=analysis_df.withColumn(\"sokei_per\",F.lag(\"sokei\",1).over(window_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 順番が上がるごとにどれだけ増えていったかみていきましょう\n",
    "\n",
    "percent.withColumn(\"percent\",percent.sokei/percent.sokei_per).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLで表現してみましょう\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "select \n",
    "*,\n",
    "sokei / LAG(sokei,1) OVER (PARTITION BY code ORDER BY cast(sokei as long) asc)  as sokei_rank\n",
    "from super_crush_course.parquet_table_with_no_partition\n",
    "\n",
    "\"\"\").show(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ピボットテーブル(と転置(transpose))\n",
    "ピボットテーブルは簡単にいうとテーブル内の項目をまとめて処理(合計、平均)することです。\n",
    "\n",
    "一般にデータベースに格納されたテーブルは\n",
    "\n",
    "- コーヒ　1 100 2022/11/11\n",
    "- コーヒ　1 100 2022/11/12\n",
    "\n",
    "といった様にレコード単位で格納されています。\n",
    "それらのデータを集計して表現してくれるものがピボットテーブル\n",
    "\n",
    "GroupByしたりすればできるのでは？\n",
    "と思われるかもしれませんが、pivotテーブルは集計のグループをヘッダーとして持つことができます。\n",
    "※うまくいえなかったので実際に見てみましょう。\n",
    "\n",
    "transposeについては次のチャプターにて紹介を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各元号の中で一番sokeiが多かったものを取り出している\n",
    "analysis_df.withColumn(\"sokei\",analysis_df.sokei.cast(\"long\")).groupBy(analysis_df.kenmei).pivot(\"gengo\").max(\"sokei\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データフレーム限定 RDDによる一行づつの操作\n",
    "\n",
    "出番がある様な、無い様な操作ですが一つSparkの特徴であるRDD(低レベル操作)についてみていきましょう。  \n",
    "Sparkは全てのDataFrameは実行されるときにRDDに変換されて実行されます(そのときに最適なRDD操作に変換してくれる)。\n",
    "\n",
    "あまり普段RDDを意識することなく操作を可能です。\n",
    "\n",
    "一応RDDでの操作も見ておきましょう。\n",
    "\n",
    "RDDに変換すると、mapとかlambdaなどPythonの関数が適用になりますが、最適化をやってくれなくなるのであまりおすすめはできないです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdds=json_df.rdd.map(lambda x: len(x.code))\n",
    "rdds.take(10)\n",
    "\n",
    "rdds.reduce(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "578f5f657c2fb65ecadb997ad60e5cf2da380ecec34305a6dd913dc5b96e257c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
