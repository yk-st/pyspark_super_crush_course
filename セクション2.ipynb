{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本セクションの目次\n",
    "1. 分散処理とは\n",
    "2. PySparkとは\n",
    "3. ノートブックとは\n",
    "4. Spark(PySpark)がデータ操作で利用するもの\n",
    "5. JSONのデータ読み込み\n",
    "6. CSVのデータ読み込み\n",
    "7. データフレームを操作する\n",
    "8. カラムナーフォーマット\n",
    "9. 圧縮形式とファイルフォーマット\n",
    "10. 行指向フォーマット\n",
    "11. パーティションとダイナミックパーティション\n",
    "12. スモールファイルとデータスキュー"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分散処理とは？\n",
    "\n",
    "分散処理とは、複数の端末にまたがって処理を行うことです。  \n",
    "スレッド処理とは異なり複数台にまたがっていることがポイントです。\n",
    "\n",
    "例えば、「This is good」「That are very good」のgoodの数を数えるプログラムを考えるときの場合を考えてみます。\n",
    "\n",
    "## スレッド処理の場合\n",
    "一台のパソコン（端末）の中でスレッドを立ち上げ処理を行います。\n",
    "\n",
    "端末1  \n",
    "　 - スレッドの呼び出しもと  \n",
    "    - スレッド1 「This is good」のgoodの数を数える  \n",
    "    - スレッド2 「That are very good」のgoodの数を数える  \n",
    "-> スレッド１、２がそれぞれgoodの数を数えてスレッドの呼び出し元にてそれぞれのスレッドの合計を取得し結果を表示する\n",
    "\n",
    "## 分散処理の場合\n",
    "複数台(今回は3台)で処理を行います。\n",
    "\n",
    "コントローラーノード(今回だとドライバー)   \n",
    "　- 端末1,端末2に指示を出して処理をさせる  \n",
    "　- 端末1,端末2からの処理結果を受け取りユーザに返却する  \n",
    "\n",
    "端末1\n",
    "  - 「This is good」のgoodの数を数える\n",
    "\n",
    "端末2\n",
    "  - 「That are very good」のgoodの数を数える\n",
    "\n",
    "分散処理の特徴は、スレッド処理としてノードを(実質)無限にスケールすることが可能な点です。  \n",
    "仮にCPUやメモリが足りなければ端末3,4,,,,と増やしていくことで処理のボトルネックを解決することが可能になるという理論です。  \n",
    "一方で、スレッド処理は一つの端末内で実行されるため、無限にスケールすることができません(CPU/メモリに限界がある)  \n",
    "\n",
    "今回のコースは、データも小さく端末が一台(コントローラーノードと端末1が同居している状態)ですが実際の環境で構築する(された)環境を利用する際は複数台であることが一般的です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySparkとは？\n",
    "PySparkとは分散処理を実現するエンジン(フレームワークの様なもの)です。  \n",
    "ビッグデータの世界では、一つのノードで処理を行うということはほとんどなく複数台のノードで一つの仕事を行い結果を出すということがしばしば行われます。  \n",
    "\n",
    "その処理を比較的簡単に行なってくれるのがSparkであり、Pythonとの組み合わせて使うことでPySparkと呼ばれています。  \n",
    "他にはScalaとの組み合わせやJavaとの組み合わせが可能ですが、ビッグデータ分析を行うという観点からPythonが多く利用されています。\n",
    "\n",
    "単純に複数台並べて多くのリクエストを捌く様なWebシステムというわけではなくて、一つの大きな仕事をを処理するためのフレームワークと考えると良いと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ちょっとだけSpark Internal\n",
    "ちょっとだけInternalということで、Sparkで利用される言葉について整理をしていきましょう。\n",
    "\n",
    "Sparkは大きく分けると\n",
    "- ドライバー\n",
    "- エグゼキューター\n",
    "\n",
    "に分かれています。\n",
    "\n",
    "ドライバーはユーザからの処理(SQLの実行など)を受け付けエグゼキューターに引き渡します。  \n",
    "そして、エグゼキュータはその処理を実行するという流れです。  \n",
    "\n",
    "ドライバーはAノードエグゼキューターはA,B、Cノードといった様に分散処理を実現していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ノートブックとは？\n",
    "\n",
    "ノートブックとはGUIで視覚的に分析を可能にした、まさにノートの様な分析環境です。  \n",
    "プログラムやこの様にマークダウンを記載する場所を「セル」と呼びます。  \n",
    "セルにプログラムやその説明を記載しながら状況を残していきます。\n",
    "\n",
    "分析の結果をノートの様に一個ずつ残していけるからノートブックと呼ばれています。\n",
    "\n",
    "このノートブック自体は他の人に共有することも可能で、実行したグラフの結果などをそのまま表示させてGitにコミットを行うことによって  \n",
    "共有を行います。\n",
    "\n",
    "今回はVSCodeのノートブック環境を利用していますが他にも\n",
    "\n",
    "- jupyter notebook\n",
    "- EMR notebook\n",
    "- zeppeline\n",
    "\n",
    "などなど、クラウド環境で提供されているものもあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark(PySpark)がデータ操作で利用するもの\n",
    "\n",
    "Sparkがデータを操作するときに利用するものは2つ(正確には3つ)あります。  \n",
    "\n",
    "- SQL\n",
    "- DataFrame\n",
    "- RDD\n",
    "\n",
    "今回のコースは、SQLとDataFrameをそれぞれ対比させながら紹介をおこなっていこうと思います。  \n",
    "また、あまり出番はないのですが、RDDについても少し触れてみようと思います。\n",
    "\n",
    "SQLとはデータを読み込みし、そのデータに対して仮想的なテーブルを作成しSQLを使ってデータ操作を行います。  \n",
    "DataFrameとはデータを読み込みし、そのデータに対してプログラムチックにデータ操作を行います。  \n",
    "RDDとはデータを読み込みし、そのデータに対してプログラムチックにデータ操作を行います(DataFrameよりより、低レベルのAPIを提供します)。\n",
    "\n",
    "Sparkには2つの読み込みタイプが存在しています  \n",
    "- スキーマオンリード(事前のテーブル定義がなくてもデータを読み込んで処理が可能)  \n",
    "- スキーマオンライト(データを読み込むためには、事前のテーブル定義が必要)  \n",
    "\n",
    "既存のテーブルからデータを読み取ることも可能ですしなくても問題ありません。\n",
    "\n",
    "詳しくはセクション3でみていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# どのセクションでも登場する前準備\n",
    "\n",
    "どのセクションでも、データ処理を開始する前のおまじないがあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンソールで設定したSparkとNoteBookを接続します(動かす前に毎度実行する必要があります)\n",
    "import findspark\n",
    "findspark.init(\"/home/pyspark/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition.mode\n",
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/pyspark/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/pyspark/spark-3.2.0-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/pyspark/.ivy2/cache\n",
      "The jars for the packages stored in: /home/pyspark/.ivy2/jars\n",
      "org.apache.spark#spark-streaming_2.13 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6220e1ed-e990-4ea8-9665-e724ff0c573f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.2.0 in central\n",
      "\tfound org.tukaani#xz;1.8 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.2.0/spark-sql-kafka-0-10_2.12-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0!spark-sql-kafka-0-10_2.12.jar (393ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.2.0/spark-avro_2.12-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-avro_2.12;3.2.0!spark-avro_2.12.jar (203ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.2.0/spark-token-provider-kafka-0-10_2.12-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0!spark-token-provider-kafka-0-10_2.12.jar (138ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.8.0/kafka-clients-2.8.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.8.0!kafka-clients.jar (2558ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (266ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.6.2/commons-pool2-2.6.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.6.2!commons-pool2.jar (190ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (170ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.1/hadoop-client-runtime-3.3.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.1!hadoop-client-runtime.jar (13896ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.7.1!lz4-java.jar (402ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.4!snappy-java.jar(bundle) (1309ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (161ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.1/hadoop-client-api-3.3.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.1!hadoop-client-api.jar (9949ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.htrace#htrace-core4;4.1.0-incubating!htrace-core4.jar (585ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (177ms)\n",
      "downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.8/xz-1.8.jar ...\n",
      "\t[SUCCESSFUL ] org.tukaani#xz;1.8!xz.jar (177ms)\n",
      ":: resolution report :: resolve 29345ms :: artifacts dl 30583ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.tukaani#xz;1.8 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   15  |   15  |   15  |   0   ||   15  |   15  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6220e1ed-e990-4ea8-9665-e724ff0c573f\n",
      "\tconfs: [default]\n",
      "\t15 artifacts copied, 0 already retrieved (59469kB/74ms)\n",
      "22/01/06 23:58:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-streaming_2.13:3.2.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1,org.apache.spark:spark-avro_2.12:3.2.1\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Spark SessionはJavaでいうインスタンスの様なもので、spark(=分散処理しますよ)というマークだと考えればOK\n",
    "# それ以外はconfigで細かな設定が可能\n",
    "# パッケージを複数渡したい時は「,」で繋いで渡します。\n",
    "# Sparkのバージョンにしっかりと合わせます(今回はSparkのバージョンが3.2を使っています。)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jsonのデータ読み込み\n",
    "SparkでJsonのデータを読み込んでみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+------------+----------+------+-------+--------+------+\n",
      "| chu|code|gengo|jinko_female|jinko_male|kenmei|seireki|   sokei|wareki|\n",
      "+----+----+-----+------------+----------+------+-------+--------+------+\n",
      "|null|  00| 大正|    27918868|  28044185|  全国|   1920|55963053|     9|\n",
      "|null|  01| 大正|     1114861|   1244322|北海道|   1920| 2359183|     9|\n",
      "|null|  02| 大正|      375161|    381293|青森県|   1920|  756454|     9|\n",
      "|null|  03| 大正|      424471|    421069|岩手県|   1920|  845540|     9|\n",
      "|null|  04| 大正|      476459|    485309|宮城県|   1920|  961768|     9|\n",
      "|null|  05| 大正|      444855|    453682|秋田県|   1920|  898537|     9|\n",
      "|null|  06| 大正|      490597|    478328|山形県|   1920|  968925|     9|\n",
      "|null|  07| 大正|      689225|    673525|福島県|   1920| 1362750|     9|\n",
      "|null|  08| 大正|      688272|    662128|茨城県|   1920| 1350400|     9|\n",
      "|null|  09| 大正|      532224|    514255|栃木県|   1920| 1046479|     9|\n",
      "+----+----+-----+------------+----------+------+-------+--------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Jsonデータの読み込み\n",
    "json_df=spark.read.json(\"./dataset/jinko.json\")\n",
    "json_df.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSVのデータ読み込み\n",
    "CSV(TSV)の読み込みは非常にパターンが多いです。\n",
    "ひとつを例にとってパターンを見てみましょう。\n",
    "\n",
    "多すぎるので全て、紹介できませんがいくつかオプションをみていきましょう。\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-csv.html\n",
    "\n",
    "- inferSchema(型をデータの中から推論するかどうか？推論なのでデータの中身によって毎度変わる可能性があるので注意)\n",
    "- lineSep(sep),(csvであれば','でtsvであればタブ)\n",
    "- header(一行目をカラム行としてみなすかどうか？)\n",
    "- multiline(カラムで改行されたりした場合に、それを一行としてみるかどうか)\n",
    "- Schema(スーキーマを設定する)\n",
    "- encoding(読み込むエンコードを設定する)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+----+----------+----------+----+------------+----------+----------+\n",
      "|都道府県コード|都道府県名|元号|和暦（年）|西暦（年）|  注|人口（総数）|人口（男）|人口（女）|\n",
      "+--------------+----------+----+----------+----------+----+------------+----------+----------+\n",
      "|            00|      全国|大正|         9|      1920|null|    55963053|  28044185|  27918868|\n",
      "|            01|    北海道|大正|         9|      1920|null|     2359183|   1244322|   1114861|\n",
      "|            02|    青森県|大正|         9|      1920|null|      756454|    381293|    375161|\n",
      "|            03|    岩手県|大正|         9|      1920|null|      845540|    421069|    424471|\n",
      "|            04|    宮城県|大正|         9|      1920|null|      961768|    485309|    476459|\n",
      "|            05|    秋田県|大正|         9|      1920|null|      898537|    453682|    444855|\n",
      "|            06|    山形県|大正|         9|      1920|null|      968925|    478328|    490597|\n",
      "|            07|    福島県|大正|         9|      1920|null|     1362750|    673525|    689225|\n",
      "|            08|    茨城県|大正|         9|      1920|null|     1350400|    662128|    688272|\n",
      "|            09|    栃木県|大正|         9|      1920|null|     1046479|    514255|    532224|\n",
      "|            10|    群馬県|大正|         9|      1920|null|     1052610|    514106|    538504|\n",
      "|            11|    埼玉県|大正|         9|      1920|null|     1319533|    641161|    678372|\n",
      "|            12|    千葉県|大正|         9|      1920|null|     1336155|    656968|    679187|\n",
      "|            13|    東京都|大正|         9|      1920|null|     3699428|   1952989|   1746439|\n",
      "|            14|  神奈川県|大正|         9|      1920|null|     1323390|    689751|    633639|\n",
      "|            15|    新潟県|大正|         9|      1920|null|     1776474|    871532|    904942|\n",
      "|            16|    富山県|大正|         9|      1920|null|      724276|    354775|    369501|\n",
      "|            17|    石川県|大正|         9|      1920|null|      747360|    364375|    382985|\n",
      "|            18|    福井県|大正|         9|      1920|null|      599155|    293181|    305974|\n",
      "|            19|    山梨県|大正|         9|      1920|null|      583453|    290817|    292636|\n",
      "+--------------+----------+----+----------+----------+----+------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#データソースの読み込み\n",
    "#sep='\\t'とすればtsvでも読み込みが可能です\n",
    "#multiLineは、CSVやTSVの各カラムに改行が含まれていた時の対策です。\n",
    "df=spark.read.option(\"multiLine\", \"true\").option(\"encoding\", \"SJIS\").csv(\"./dataset/jinko.csv\", header=True, sep=',', inferSchema=False)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-----+----------+----------+----+------------+----------+------------+\n",
      "|code          |kenmei    |gengo|wareki    |seireki   |chu |sokei       |jinko_male|jinko_female|\n",
      "+--------------+----------+-----+----------+----------+----+------------+----------+------------+\n",
      "|都道府県コード|都道府県名|元号 |和暦（年）|西暦（年）|注  |人口（総数）|人口（男）|人口（女）  |\n",
      "|00            |全国      |大正 |9         |1920      |null|55963053    |28044185  |27918868    |\n",
      "|01            |北海道    |大正 |9         |1920      |null|2359183     |1244322   |1114861     |\n",
      "|02            |青森県    |大正 |9         |1920      |null|756454      |381293    |375161      |\n",
      "+--------------+----------+-----+----------+----------+----+------------+----------+------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+----+------+-----+------+-------+----+--------+----------+------------+\n",
      "|code|kenmei|gengo|wareki|seireki|chu |sokei   |jinko_male|jinko_female|\n",
      "+----+------+-----+------+-------+----+--------+----------+------------+\n",
      "|00  |全国  |大正 |9     |1920   |null|55963053|28044185  |27918868    |\n",
      "|01  |北海道|大正 |9     |1920   |null|2359183 |1244322   |1114861     |\n",
      "|02  |青森県|大正 |9     |1920   |null|756454  |381293    |375161      |\n",
      "|03  |岩手県|大正 |9     |1920   |null|845540  |421069    |424471      |\n",
      "+----+------+-----+------+-------+----+--------+----------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#スキーマ設定をしていきましょう\n",
    "struct = StructType([\n",
    "    StructField(\"code\", StringType(), False),\n",
    "    StructField(\"kenmei\", StringType(), False),\n",
    "    StructField(\"gengo\", StringType(), False),\n",
    "    StructField(\"wareki\", StringType(), False),\n",
    "    StructField(\"seireki\", StringType(), False),\n",
    "    StructField(\"chu\", StringType(), False),\n",
    "    StructField(\"sokei\", StringType(), False),\n",
    "    StructField(\"jinko_male\", StringType(), False),\n",
    "    StructField(\"jinko_female\", StringType(), False)\n",
    "])\n",
    "\n",
    "df_csv=spark.read.option(\"multiLine\", \"true\").option(\"encoding\", \"SJIS\") \\\n",
    "    .csv(\"./dataset/jinko.csv\", header=False, sep=',', inferSchema=False, schema=struct)\n",
    "\n",
    "df_csv.show(truncate=False,n=4)\n",
    "\n",
    "df_csv=df_csv.filter(df_csv['code'] != '都道府県コード')\n",
    "df_csv.show(truncate=False,n=4)\n",
    "\n",
    "# headerやsepなどはoptionで渡すことも可能ですが、csv関数の引数として渡すことも可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データフレーム\n",
    "df_csvやjson_dfがデータフレームと呼ばれる変数\n",
    "\n",
    "データを枠組み(フレーム)として扱うことができるのがデータフレームです。  \n",
    "データフレームの作成方法は\n",
    "\n",
    "- SQLから生成する\n",
    "- データを読み込んで生成する\n",
    "- プログラム的に作成する\n",
    "\n",
    "SQLから生成はSQLで取得した結果がデータフレームで返却されるます  \n",
    "\n",
    "データを読み込んで生成する場合はCSVやJSon、Parquet、Avroなどのファイルを読み込んでデータフレームに取り込みを行うことが可能  \n",
    "\n",
    "プログラム的に作成するのは、空のデータフレームが欲しい時など\n",
    "\n",
    "## データフレームの操作\n",
    "データフレームに対する操作はたくさんあるのですが、今回はよく使う\n",
    "\n",
    "- withColumnsとalias\n",
    "- When(otherwise)\n",
    "- null関係の操作(fillna)\n",
    "- 条件絞り込み(filter)\n",
    "- Subtract\n",
    "\n",
    "について紹介していきたいと思います。\n",
    "\n",
    "集計関数や結合については別のセクションで紹介します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+------+-------+----+--------+----------+------------+----+\n",
      "|code|kenmei|gengo|wareki|seireki| chu|   sokei|jinko_male|jinko_female|hoge|\n",
      "+----+------+-----+------+-------+----+--------+----------+------------+----+\n",
      "|  00|  全国| 大正|     9|   1920|null|55963053|  28044185|    27918868|   1|\n",
      "|  01|北海道| 大正|     9|   1920|null| 2359183|   1244322|     1114861|   1|\n",
      "+----+------+-----+------+-------+----+--------+----------+------------+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+------+-----+------+-------+----+--------+----------+------------+----+\n",
      "|code|kenmei|gengo|wareki|seireki| chu|   sokei|jinko_male|jinko_female|peke|\n",
      "+----+------+-----+------+-------+----+--------+----------+------------+----+\n",
      "|  00|  全国| 大正|     9|   1920|null|55963053|  28044185|    27918868|   2|\n",
      "|  01|北海道| 大正|     9|   1920|null| 2359183|   1244322|     1114861|   2|\n",
      "+----+------+-----+------+-------+----+--------+----------+------------+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----+------+-----+------+-------+----+---------+----------+------------+----+\n",
      "|code|kenmei|gengo|wareki|seireki| chu|    sokei|jinko_male|jinko_female|peke|\n",
      "+----+------+-----+------+-------+----+---------+----------+------------+----+\n",
      "|  00|  全国| 大正|     9|   1920|null| 55963053|  28044185|    27918868|   2|\n",
      "|  00|  全国| 大正|    14|   1925|null| 59736822|  30013109|    29723713|   2|\n",
      "|  00|  全国| 昭和|     5|   1930|null| 64450005|  32390155|    32059850|   5|\n",
      "|  00|  全国| 昭和|    10|   1935|null| 69254148|  34734133|    34520015|  10|\n",
      "|  00|  全国| 昭和|    15|   1940|null| 73114308|  36566010|    36548298|  15|\n",
      "|  00|  全国| 昭和|    20|   1945|  1)| 71998104|  33894059|    38104045|  20|\n",
      "|  00|  全国| 昭和|    25|   1950|null| 84114574|  41241192|    42873382|  25|\n",
      "|  00|  全国| 昭和|    30|   1955|null| 90076594|  44242657|    45833937|  30|\n",
      "|  00|  全国| 昭和|    35|   1960|  2)| 94301623|  46300445|    48001178|  35|\n",
      "|  00|  全国| 昭和|    40|   1965|null| 99209137|  48692138|    50516999|  40|\n",
      "|  00|  全国| 昭和|    45|   1970|null|104665171|  51369177|    53295994|  45|\n",
      "|  00|  全国| 昭和|    50|   1975|null|111939643|  55090673|    56848970|  50|\n",
      "|  00|  全国| 昭和|    55|   1980|null|117060396|  57593769|    59466627|  55|\n",
      "|  00|  全国| 昭和|    60|   1985|null|121048923|  59497316|    61551607|  60|\n",
      "|  00|  全国| 平成|     2|   1990|null|123611167|  60696724|    62914443|null|\n",
      "|  00|  全国| 平成|     7|   1995|null|125570246|  61574398|    63995848|null|\n",
      "|  00|  全国| 平成|    12|   2000|null|126925843|  62110764|    64815079|null|\n",
      "|  00|  全国| 平成|    17|   2005|null|127767994|  62348977|    65419017|null|\n",
      "|  00|  全国| 平成|    22|   2010|null|128057352|  62327737|    65729615|null|\n",
      "|  00|  全国| 平成|    27|   2015|null|127094745|  61841738|    65253007|null|\n",
      "+----+------+-----+------+-------+----+---------+----------+------------+----+\n",
      "\n",
      "+----+\n",
      "|peke|\n",
      "+----+\n",
      "|  00|\n",
      "|  01|\n",
      "|  02|\n",
      "|  03|\n",
      "+----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ちなみにdf_csv.と打つと候補がたくさん出てきます\n",
    "\n",
    "# WithCloumnsはカラムを付与する作業のことです\n",
    "# CSVのデータフレームに対してカラムを付与してみましょう\n",
    "from pyspark.sql.functions import lit,when\n",
    "df_csv.withColumn(\"hoge\",lit(\"1\")).show(n=2)\n",
    "\n",
    "# when\n",
    "# IF文みたいなものです\n",
    "df_csv.withColumn(\"peke\",when(df_csv.gengo =='大正',lit(\"2\"))).show(n=2)\n",
    "\n",
    "#下の方みるとわかるのですが平成がNUllです\n",
    "df_csv.withColumn(\"peke\",when(df_csv.gengo =='大正',lit(\"2\")) \\\n",
    ".when(df_csv.gengo == '昭和',df_csv.wareki)).filter(df_csv.kenmei == '全国').show(n=20)\n",
    "\n",
    "df_csv.select(df_csv.code.alias('peke')).show(n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+------+-------+----+---------+----------+------------+-----+\n",
      "|code|kenmei|gengo|wareki|seireki| chu|    sokei|jinko_male|jinko_female| peke|\n",
      "+----+------+-----+------+-------+----+---------+----------+------------+-----+\n",
      "|  00|  全国| 大正|     9|   1920|null| 55963053|  28044185|    27918868|    2|\n",
      "|  00|  全国| 大正|    14|   1925|null| 59736822|  30013109|    29723713|    2|\n",
      "|  00|  全国| 昭和|     5|   1930|null| 64450005|  32390155|    32059850|    5|\n",
      "|  00|  全国| 昭和|    10|   1935|null| 69254148|  34734133|    34520015|   10|\n",
      "|  00|  全国| 昭和|    15|   1940|null| 73114308|  36566010|    36548298|   15|\n",
      "|  00|  全国| 昭和|    20|   1945|  1)| 71998104|  33894059|    38104045|   20|\n",
      "|  00|  全国| 昭和|    25|   1950|null| 84114574|  41241192|    42873382|   25|\n",
      "|  00|  全国| 昭和|    30|   1955|null| 90076594|  44242657|    45833937|   30|\n",
      "|  00|  全国| 昭和|    35|   1960|  2)| 94301623|  46300445|    48001178|   35|\n",
      "|  00|  全国| 昭和|    40|   1965|null| 99209137|  48692138|    50516999|   40|\n",
      "|  00|  全国| 昭和|    45|   1970|null|104665171|  51369177|    53295994|   45|\n",
      "|  00|  全国| 昭和|    50|   1975|null|111939643|  55090673|    56848970|   50|\n",
      "|  00|  全国| 昭和|    55|   1980|null|117060396|  57593769|    59466627|   55|\n",
      "|  00|  全国| 昭和|    60|   1985|null|121048923|  59497316|    61551607|   60|\n",
      "|  00|  全国| 平成|     2|   1990|null|123611167|  60696724|    62914443|99999|\n",
      "|  00|  全国| 平成|     7|   1995|null|125570246|  61574398|    63995848|99999|\n",
      "|  00|  全国| 平成|    12|   2000|null|126925843|  62110764|    64815079|99999|\n",
      "|  00|  全国| 平成|    17|   2005|null|127767994|  62348977|    65419017|99999|\n",
      "|  00|  全国| 平成|    22|   2010|null|128057352|  62327737|    65729615|99999|\n",
      "|  00|  全国| 平成|    27|   2015|null|127094745|  61841738|    65253007|99999|\n",
      "+----+------+-----+------+-------+----+---------+----------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#　平成がNullではなくないりました\n",
    "df_csv.withColumn(\"peke\",when(df_csv.gengo =='大正',lit(\"2\")) \\\n",
    ".when(df_csv.gengo == '昭和',df_csv.wareki).otherwise(\"99999\")).filter(df_csv.kenmei == '全国').show(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+------+-------+---+---------+----------+------------+----+\n",
      "|code|kenmei|gengo|wareki|seireki|chu|    sokei|jinko_male|jinko_female|peke|\n",
      "+----+------+-----+------+-------+---+---------+----------+------------+----+\n",
      "|  00|  全国| 大正|     9|   1920|  1| 55963053|  28044185|    27918868|   2|\n",
      "|  00|  全国| 大正|    14|   1925|  1| 59736822|  30013109|    29723713|   2|\n",
      "|  00|  全国| 昭和|     5|   1930|  1| 64450005|  32390155|    32059850|   5|\n",
      "|  00|  全国| 昭和|    10|   1935|  1| 69254148|  34734133|    34520015|  10|\n",
      "|  00|  全国| 昭和|    15|   1940|  1| 73114308|  36566010|    36548298|  15|\n",
      "|  00|  全国| 昭和|    20|   1945| 1)| 71998104|  33894059|    38104045|  20|\n",
      "|  00|  全国| 昭和|    25|   1950|  1| 84114574|  41241192|    42873382|  25|\n",
      "|  00|  全国| 昭和|    30|   1955|  1| 90076594|  44242657|    45833937|  30|\n",
      "|  00|  全国| 昭和|    35|   1960| 2)| 94301623|  46300445|    48001178|  35|\n",
      "|  00|  全国| 昭和|    40|   1965|  1| 99209137|  48692138|    50516999|  40|\n",
      "|  00|  全国| 昭和|    45|   1970|  1|104665171|  51369177|    53295994|  45|\n",
      "|  00|  全国| 昭和|    50|   1975|  1|111939643|  55090673|    56848970|  50|\n",
      "|  00|  全国| 昭和|    55|   1980|  1|117060396|  57593769|    59466627|  55|\n",
      "|  00|  全国| 昭和|    60|   1985|  1|121048923|  59497316|    61551607|  60|\n",
      "|  00|  全国| 平成|     2|   1990|  1|123611167|  60696724|    62914443|   a|\n",
      "|  00|  全国| 平成|     7|   1995|  1|125570246|  61574398|    63995848|   a|\n",
      "|  00|  全国| 平成|    12|   2000|  1|126925843|  62110764|    64815079|   a|\n",
      "|  00|  全国| 平成|    17|   2005|  1|127767994|  62348977|    65419017|   a|\n",
      "|  00|  全国| 平成|    22|   2010|  1|128057352|  62327737|    65729615|   a|\n",
      "|  00|  全国| 平成|    27|   2015|  1|127094745|  61841738|    65253007|   a|\n",
      "+----+------+-----+------+-------+---+---------+----------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# nullを処理する方法は他にもあります\n",
    "# fillna\n",
    "df_csv.withColumn(\"peke\",when(df_csv.gengo =='大正',lit(\"2\")) \\\n",
    ".when(df_csv.gengo == '昭和',df_csv.wareki)).fillna({\"peke\":\"a\",\"chu\":\"1\"}).filter(df_csv.kenmei == '全国').show(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----+------+-------+----+-------+----------+------------+\n",
      "|code|  kenmei|gengo|wareki|seireki| chu|  sokei|jinko_male|jinko_female|\n",
      "+----+--------+-----+------+-------+----+-------+----------+------------+\n",
      "|  30|和歌山県| 昭和|    55|   1980|null|1087012|    523467|      563545|\n",
      "|  05|  秋田県| 平成|     7|   1995|null|1213667|    577535|      636132|\n",
      "|  35|  山口県| 平成|    17|   2005|null|1492606|    703721|      788885|\n",
      "|  25|  滋賀県| 昭和|     5|   1930|null| 691631|    337016|      354615|\n",
      "|  26|  京都府| 昭和|     5|   1930|null|1552832|    792420|      760412|\n",
      "|  47|  沖縄県| 昭和|    25|   1950|null| 914937|    429432|      485505|\n",
      "|  16|  富山県| 平成|     7|   1995|null|1123125|    540921|      582204|\n",
      "|  02|  青森県| 平成|    12|   2000|null|1475728|    702573|      773155|\n",
      "|  23|  愛知県| 平成|    27|   2015|null|7483128|   3740844|     3742284|\n",
      "|  30|和歌山県| 大正|    14|   1925|null| 787511|    392191|      395320|\n",
      "|  20|  長野県| 昭和|     5|   1930|null|1717118|    832312|      884806|\n",
      "|  27|  大阪府| 昭和|     5|   1930|null|3540017|   1845786|     1694231|\n",
      "|  34|  広島県| 昭和|    15|   1940|null|1869504|    936936|      932568|\n",
      "|  35|  山口県| 平成|    12|   2000|null|1527964|    722683|      805281|\n",
      "|  16|  富山県| 昭和|    10|   1935|null| 798890|    388771|      410119|\n",
      "|  04|  宮城県| 昭和|    20|   1945|  1)|1462254|    684453|      777801|\n",
      "|  05|  秋田県| 昭和|    55|   1980|null|1256745|    603403|      653342|\n",
      "|  16|  富山県| 平成|    22|   2010|null|1093247|    526605|      566642|\n",
      "|  02|  青森県| 平成|    27|   2015|null|1308265|    614694|      693571|\n",
      "|  42|  長崎県| 昭和|    20|   1945|  1)|1318589|    618861|      699728|\n",
      "+----+--------+-----+------+-------+----+-------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 最後はSubtract\n",
    "# データの一致などを計算する際に利用されることが多いです。\n",
    "# 例えばデータ移行などで、移行前と移行後でデータを比較したい時など\n",
    "\n",
    "df_csv.subtract(json_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# カラムナーフォーマット\n",
    "ここまでで読み込んだJsonやCsvはローデータとよばれる、データ基盤では扱いづらい部類に入るファイルです。  \n",
    "扱いづらい理由はファイルがスプリッタブルかどうか？という点が関わっています。  \n",
    "\n",
    "スプリッタブルであればデータを複数端末で分割して処理することが可能ですし、そうでなければ分割して処理をすることができないため非効率が発生します。  \n",
    "\n",
    "カラムナーフォーマットと次のレクチャーで紹介するAvro(行指向フォーマット)はいずれもスプリッタブルなファイルです。  \n",
    "そのため、CSVやJSON形式のままデータ基盤で活用を続けるのは悪手なので、必ずカラムナーフォーマット、行指向フォーマットへ変換する処理を挟みましょう。  \n",
    "\n",
    "カラムナーフォーマットとは、分析に特化したファイル形式です。  \n",
    "列ごとにデータをまとめて保存することによって、データを圧縮し検索速度(特にGroup byなどの集計構文)をあげています。  \n",
    "また、カラムナーフォーマットは内部にカラム情報を保持しているためセクション2で紹介したスキーマオンリード機能を使ってスキーマ定義を読み出すことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 圧縮形式とファイルフォーマット\n",
    "\n",
    "前のレクチャーではスプリッタブルかどうかが重要であるとお伝えしました。  \n",
    "ここで一度、ファイルと圧縮形式の組み合わせについてみていきましょう。\n",
    "\n",
    "```\n",
    "        paruqet avro csv/json\n",
    "gz       Y      Y      N\n",
    "snappy   Y      Y      -\n",
    "bz2      -      -      Y\n",
    "圧縮なし  Y      Y      N\n",
    "\n",
    "```\n",
    "\n",
    "大事なのは、csvやjsonの場合はbz2を利用しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 行指向フォーマット\n",
    "先ほどはカラムナーフォーマットを紹介しましたが、次は行指向フォーマットを紹介について紹介していきます。  \n",
    "\n",
    "よく利用されているRDBも行指向の仕組みを持ったデータシステムです。  \n",
    "RDBはレコードの追加が得意です。  \n",
    " \n",
    "そして、ビッグデータはレコードを一件つづ追加していく様な処理が苦手でしたが行指向フォーマットのAvroの登場によってレコードの追加が頻繁に発生する処理  \n",
    "つまりストリーミング処理にも適用することが可能になりました。  \n",
    "\n",
    "まず覚えてほしいのは、Avroはメインとしてストリーミング処理の利用に向いているということです（バッチ処理にも使うことが可能です）  \n",
    "\n",
    "\n",
    "ビッグデータにおけるストリーミングについて詳しく知りたい方は  \n",
    "「データサイエンスのためのストリーミング前処理入門　PythonとSparkで始めるビッグデータストリーミング処理入門」\n",
    "を是非受講ください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パーティションとダイナミックパーティション\n",
    "\n",
    "データはパーティションと呼ばれれる区切りにデータを保存していくことが可能です。\n",
    "\n",
    "```\n",
    "テーブル\n",
    "    パーティション1\n",
    "        パーティション1に関するデータ\n",
    "\n",
    "    パーティション2\n",
    "        パーティション2に関するデータ\n",
    "```\n",
    "\n",
    "今回は、パーティションとしてkenmeiを利用して保存をしていこうと思います。\n",
    "また、ファイルの出力としてParquet/Avroでそれぞれ出力を行ってみたいと思います。\n",
    "\n",
    "## ダイナミックパーティション\n",
    "ダイナミックパーティションとはデータをもとにパーティションを作成することを指します。  \n",
    "Hiveなどで使われる言葉ですが、データをもとに振り分けるんだな。  \n",
    "とだけ理解しておけばいいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# repartitionはパーティション配下のファイルの数をまとめるものです\n",
    "# 次のスモールデータとデータスキューネスに関係していきます。\n",
    "\n",
    "\n",
    "# Parquet\n",
    "# パーティションなしの場合\n",
    "df_csv.repartition(2).write.mode('overwrite').parquet(\"/home/pyspark/super_crush_course.db/parquet_table_with_no_partition\")\n",
    "\n",
    "\n",
    "# Avroでファイルを出力してみよう\n",
    "# パーティションなしの場合\n",
    "df_csv.repartition(2).write.mode('overwrite').format(\"avro\").save(\"/home/pyspark/super_crush_course.db/avro_table_with_no_partition\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# パーティションありの場合\n",
    "df_csv.repartition(2).write.partitionBy('kenmei').mode('overwrite').parquet(\"/home/pyspark/super_crush_course.db/parquet_table_with_partition\")\n",
    "\n",
    "# パーティションありの場合\n",
    "df_csv.repartition(2).write.partitionBy('kenmei').mode('overwrite').format(\"avro\").save(\"/home/pyspark/super_crush_course.db/avro_table_with_partition\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 52\n",
      "drwxr-xr-x 2 pyspark pyspark  4096 Jan  7 02:10 .\n",
      "drwxr-xr-x 6 pyspark pyspark  4096 Jan  7 02:10 ..\n",
      "-rw-r--r-- 1 pyspark pyspark     8 Jan  7 02:10 ._SUCCESS.crc\n",
      "-rw-r--r-- 1 pyspark pyspark   136 Jan  7 02:10 .part-00000-d7641e03-9b73-452a-9dab-f09056a6698a-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 pyspark pyspark   136 Jan  7 02:10 .part-00001-d7641e03-9b73-452a-9dab-f09056a6698a-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 pyspark pyspark     0 Jan  7 02:10 _SUCCESS\n",
      "-rw-r--r-- 1 pyspark pyspark 16165 Jan  7 02:10 part-00000-d7641e03-9b73-452a-9dab-f09056a6698a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 pyspark pyspark 16288 Jan  7 02:10 part-00001-d7641e03-9b73-452a-9dab-f09056a6698a-c000.snappy.parquet\n",
      "==============\n",
      "total 216\n",
      "drwxr-xr-x 53 pyspark pyspark 4096 Jan  7 02:10  .\n",
      "drwxr-xr-x  6 pyspark pyspark 4096 Jan  7 02:10  ..\n",
      "-rw-r--r--  1 pyspark pyspark    8 Jan  7 02:10  ._SUCCESS.crc\n",
      "-rw-r--r--  1 pyspark pyspark    0 Jan  7 02:10  _SUCCESS\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=__HIVE_DEFAULT_PARTITION__'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=三重県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=京都府'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=人口集中地区'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=人口集中地区以外の地区'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=佐賀県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=全国'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=兵庫県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=北海道'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=千葉県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=和歌山県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=埼玉県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=大分県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=大阪府'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=奈良県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=宮城県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=宮崎県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=富山県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=山口県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=山形県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=山梨県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=岐阜県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=岡山県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=岩手県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=島根県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=広島県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=徳島県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=愛媛県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=愛知県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=新潟県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=東京都'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=栃木県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=沖縄県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=滋賀県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=熊本県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=石川県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=神奈川県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=福井県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=福岡県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=福島県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=秋田県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=群馬県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=茨城県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=長崎県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=長野県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=青森県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=静岡県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=香川県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=高知県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=鳥取県'\n",
      "drwxr-xr-x  2 pyspark pyspark 4096 Jan  7 02:10 'kenmei=鹿児島県'\n"
     ]
    }
   ],
   "source": [
    "!ls -al /home/pyspark/super_crush_course.db/parquet_table_with_no_partition\n",
    "!echo '=============='\n",
    "!ls -al /home/pyspark/super_crush_course.db/parquet_table_with_partition/kenmei=三重県"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スモールデータとデータスキューネス\n",
    "ここで一点気をつけなければならないのが、スモールファイルとデータスキューネスです。\n",
    "\n",
    "ビッグデータの世界では、データが小さすぎると処理のボトルネックになります。\n",
    "\n",
    "前のレクチャーのパーティションに分けることは確かに有効な手段なのですが、あまりに小さくパーティションを作成しすぎるとファイルのサイズが小さくなってしまうため  \n",
    "パーティションお分け方には要注意です。\n",
    "\n",
    "また、データの偏り(スキュー)にも気をつける必要があります。  \n",
    "Aのパーティションだけが異常にデータ量が大きくBのパーティションが異常にデータが小さい場合は処理の遅延を招く原因の一つでもあります。\n",
    "\n",
    "そのため、パーティションの設計は適切に行う必要があります。  \n",
    "良かれと思ってパーティションを綺麗に切ったつもりが実は処理の低下を招いていたということもあるのです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "578f5f657c2fb65ecadb997ad60e5cf2da380ecec34305a6dd913dc5b96e257c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
