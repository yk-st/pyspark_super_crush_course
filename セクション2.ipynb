{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本セクションの目次\n",
    "1. Avroフォーマット\n",
    "2. 前方互換と後方互換と完全互換\n",
    "3. メッセージキューとAvroを連携してみよう\n",
    "4. Avroファイルの読み書き\n",
    "5. Avroで前方互換をやってみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySparkとは？\n",
    "PySparkとは分散処理を実現するエンジン(フレームワークの様なもの)です。  \n",
    "ビッグデータの世界では、一つのノードで処理を行うということはほとんどなく複数台のノードで一つの仕事を行い結果を出すということがしばしば行われます。  \n",
    "\n",
    "その処理を比較的簡単に行なってくれるのがSparkであり、Pythonとの組み合わせて使うことでPySparkと呼ばれています。  \n",
    "他にはScalaとの組み合わせやJavaとの組み合わせが可能ですが、ビッグデータ分析を行うという観点からPythonが多く利用されています。\n",
    "\n",
    "単純に複数台並べて多くのリクエストを捌く様なWebシステムというわけではなくて、一つの大きな仕事をを処理するためのフレームワークと考えると良いと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分散処理とは？\n",
    "\n",
    "分散処理とは、複数の端末にまたがって処理を行うことです。  \n",
    "スレッド処理とは異なり複数台にまたがっていることがポイントです。\n",
    "\n",
    "例えば、「This is good」「That are very good」のgoodの数を数えるプログラムを考えるときの場合を考えてみます。\n",
    "\n",
    "## スレッド処理の場合\n",
    "一台のパソコン（端末）の中でスレッドを立ち上げ処理を行います。\n",
    "\n",
    "端末1  \n",
    "　 - スレッドの呼び出しもと  \n",
    "    - スレッド1 「This is good」のgoodの数を数える  \n",
    "    - スレッド2 「That are very good」のgoodの数を数える  \n",
    "-> スレッド１、２がそれぞれgoodの数を数えてスレッドの呼び出し元にてそれぞれのスレッドの合計を取得し結果を表示する\n",
    "\n",
    "## 分散処理の場合\n",
    "複数台(今回は3台)で処理を行います。\n",
    "\n",
    "コントローラーノード   \n",
    "　- 端末1,端末2に指示を出して処理をさせる  \n",
    "　- 端末1,端末2からの処理結果を受け取りユーザに返却する  \n",
    "\n",
    "端末1\n",
    "  - 「This is good」のgoodの数を数える\n",
    "\n",
    "端末2\n",
    "  - 「That are very good」のgoodの数を数える\n",
    "\n",
    "分散処理の特徴は、スレッド処理としてノードを(実質)無限にスケールすることが可能な点です。  \n",
    "仮にCPUやメモリが足りなければ端末3,4,,,,と増やしていくことで処理のボトルネックを解決することが可能になるという理論です。  \n",
    "一方で、スレッド処理は一つの端末内で実行されるため、無限にスケールすることができません(CPU/メモリに限界がある)  \n",
    "\n",
    "今回のコースは、データも小さく端末が一台(コントローラーノードと端末1が同居している状態)ですが実際の環境で構築する(された)環境を利用する際は複数台であることが一般的です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ノートブックとは？\n",
    "\n",
    "ノートブックとはGUIで視覚的に分析を可能にした、まさにノートの様な分析環境です。  \n",
    "プログラムやこの様にマークダウンを記載する場所を「セル」と呼びます。  \n",
    "セルにプログラムやその説明を記載しながら状況を残していきます。\n",
    "\n",
    "分析の結果をノートの様に一個ずつ残していけるからノートブックと呼ばれています。\n",
    "\n",
    "このノートブック自体は他の人に共有することも可能で、実行したグラフの結果などをそのまま表示させてGitにコミットを行うことによって  \n",
    "共有を行います。\n",
    "\n",
    "今回はVSCodeのノートブック環境を利用していますが他にも\n",
    "\n",
    "- jupyter notebook\n",
    "- EMR notebook\n",
    "- zeppeline\n",
    "\n",
    "などなど、クラウド環境で提供されているものもあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark(PySpark)がデータ操作で利用するもの\n",
    "\n",
    "Sparkがデータを操作するときに利用するものは2つ(正確には3つ)あります。  \n",
    "\n",
    "- SQL\n",
    "- DataFrame\n",
    "- RDD\n",
    "\n",
    "今回のコースは、SQLとDataFrameをそれぞれ対比させながら紹介をおこなっていこうと思います。  \n",
    "また、あまり出番はないのですが、RDDについても少し触れてみようと思います。\n",
    "\n",
    "SQLとはデータを読み込みし、そのデータに対して仮想的なテーブルを作成しSQLを使ってデータ操作を行います。  \n",
    "DataFrameとはデータを読み込みし、そのデータに対してプログラムチックにデータ操作を行います。  \n",
    "RDDとはデータを読み込みし、そのデータに対してプログラムチックにデータ操作を行います(DataFrameよりより、低レベルのAPIを提供します)。\n",
    "\n",
    "Sparkには2つの読み込みタイプが存在しています  \n",
    "- スキーマオンリード(事前のテーブル定義がなくてもデータを読み込んで処理が可能)  \n",
    "- スキーマオンライト(データを読み込むためには、事前のテーブル定義が必要)  \n",
    "\n",
    "既存のテーブルからデータを読み取ることも可能ですしなくても問題ありません。\n",
    "\n",
    "詳しくはセクション3でみていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# どのセクションでも登場する前準備\n",
    "\n",
    "どのセクションでも、データ処理を開始する前のおまじないがあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンソールで設定したSparkとNoteBookを接続します(動かす前に毎度実行する必要があります)\n",
    "import findspark\n",
    "findspark.init(\"/home/pyspark/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-streaming_2.13:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-avro_2.12:3.2.0\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Spark SessionはJavaでいうインスタンスの様なもので、spark(=分散処理しますよ)というマークだと考えればOK\n",
    "# それ以外はconfigで細かな設定が可能\n",
    "# パッケージを複数渡したい時は「,」で繋いで渡します。\n",
    "# Sparkのバージョンにしっかりと合わせます(今回はSparkのバージョンが3.2を使っています。)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "578f5f657c2fb65ecadb997ad60e5cf2da380ecec34305a6dd913dc5b96e257c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
