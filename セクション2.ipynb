{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本セクションの目次\n",
    "1. Avroフォーマット\n",
    "2. 前方互換と後方互換と完全互換\n",
    "3. メッセージキューとAvroを連携してみよう\n",
    "4. Avroファイルの読み書き\n",
    "5. Avroで前方互換をやってみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySparkとは？\n",
    "PySparkとは分散処理を実現するエンジン(フレームワークの様なもの)です。  \n",
    "ビッグデータの世界では、一つのノードで処理を行うということはほとんどなく複数台のノードで一つの仕事を行い結果を出すということがしばしば行われます。  \n",
    "\n",
    "その処理を比較的簡単に行なってくれるのがSparkであり、Pythonとの組み合わせて使うことでPySparkと呼ばれています。  \n",
    "他にはScalaとの組み合わせやJavaとの組み合わせが可能ですが、ビッグデータ分析を行うという観点からPythonが多く利用されています。\n",
    "\n",
    "単純に複数台並べて多くのリクエストを捌く様なWebシステムというわけではなくて、一つの大きな仕事をを処理するためのフレームワークと考えると良いと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分散処理とは？\n",
    "\n",
    "分散処理とは、複数の端末にまたがって処理を行うことです。  \n",
    "スレッド処理とは異なり複数台にまたがっていることがポイントです。\n",
    "\n",
    "例えば、「This is good」「That are very good」のgoodの数を数えるプログラムを考えるときの場合を考えてみます。\n",
    "\n",
    "## スレッド処理の場合\n",
    "一台のパソコン（端末）の中でスレッドを立ち上げ処理を行います。\n",
    "\n",
    "端末1  \n",
    "　 - スレッドの呼び出しもと  \n",
    "    - スレッド1 「This is good」のgoodの数を数える  \n",
    "    - スレッド2 「That are very good」のgoodの数を数える  \n",
    "-> スレッド１、２がそれぞれgoodの数を数えてスレッドの呼び出し元にてそれぞれのスレッドの合計を取得し結果を表示する\n",
    "\n",
    "## 分散処理の場合\n",
    "複数台(今回は3台)で処理を行います。\n",
    "\n",
    "コントローラーノード   \n",
    "　- 端末1,端末2に指示を出して処理をさせる  \n",
    "　- 端末1,端末2からの処理結果を受け取りユーザに返却する  \n",
    "\n",
    "端末1\n",
    "  - 「This is good」のgoodの数を数える\n",
    "\n",
    "端末2\n",
    "  - 「That are very good」のgoodの数を数える\n",
    "\n",
    "分散処理の特徴は、スレッド処理としてノードを(実質)無限にスケールすることが可能な点です。  \n",
    "仮にCPUやメモリが足りなければ端末3,4,,,,と増やしていくことで処理のボトルネックを解決することが可能になるという理論です。  \n",
    "一方で、スレッド処理は一つの端末内で実行されるため、無限にスケールすることができません(CPU/メモリに限界がある)  \n",
    "\n",
    "今回のコースは、データも小さく端末が一台(コントローラーノードと端末1が同居している状態)ですが実際の環境で構築する(された)環境を利用する際は複数台であることが一般的です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ノートブックとは？\n",
    "\n",
    "ノートブックとはGUIで視覚的に分析を可能にした、まさにノートの様な分析環境です。  \n",
    "プログラムやこの様にマークダウンを記載する場所を「セル」と呼びます。  \n",
    "セルにプログラムやその説明を記載しながら状況を残していきます。\n",
    "\n",
    "分析の結果をノートの様に一個ずつ残していけるからノートブックと呼ばれています。\n",
    "\n",
    "このノートブック自体は他の人に共有することも可能で、実行したグラフの結果などをそのまま表示させてGitにコミットを行うことによって  \n",
    "共有を行います。\n",
    "\n",
    "今回はVSCodeのノートブック環境を利用していますが他にも\n",
    "\n",
    "- jupyter notebook\n",
    "- EMR notebook\n",
    "- zeppeline\n",
    "\n",
    "などなど、クラウド環境で提供されているものもあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark(PySpark)がデータ操作で利用するもの\n",
    "\n",
    "Sparkがデータを操作するときに利用するものは2つ(正確には3つ)あります。  \n",
    "\n",
    "- SQL\n",
    "- DataFrame\n",
    "- RDD\n",
    "\n",
    "今回のコースは、SQLとDataFrameをそれぞれ対比させながら紹介をおこなっていこうと思います。  \n",
    "また、あまり出番はないのですが、RDDについても少し触れてみようと思います。\n",
    "\n",
    "SQLとはデータを読み込みし、そのデータに対して仮想的なテーブルを作成しSQLを使ってデータ操作を行います。  \n",
    "DataFrameとはデータを読み込みし、そのデータに対してプログラムチックにデータ操作を行います。  \n",
    "RDDとはデータを読み込みし、そのデータに対してプログラムチックにデータ操作を行います(DataFrameよりより、低レベルのAPIを提供します)。\n",
    "\n",
    "Sparkには2つの読み込みタイプが存在しています  \n",
    "- スキーマオンリード(事前のテーブル定義がなくてもデータを読み込んで処理が可能)  \n",
    "- スキーマオンライト(データを読み込むためには、事前のテーブル定義が必要)  \n",
    "\n",
    "既存のテーブルからデータを読み取ることも可能ですしなくても問題ありません。\n",
    "\n",
    "詳しくはセクション3でみていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# どのセクションでも登場する前準備\n",
    "\n",
    "どのセクションでも、データ処理を開始する前のおまじないがあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンソールで設定したSparkとNoteBookを接続します(動かす前に毎度実行する必要があります)\n",
    "import findspark\n",
    "findspark.init(\"/home/pyspark/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-streaming_2.13:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-avro_2.12:3.2.0\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Spark SessionはJavaでいうインスタンスの様なもので、spark(=分散処理しますよ)というマークだと考えればOK\n",
    "# それ以外はconfigで細かな設定が可能\n",
    "# パッケージを複数渡したい時は「,」で繋いで渡します。\n",
    "# Sparkのバージョンにしっかりと合わせます(今回はSparkのバージョンが3.2を使っています。)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jsonのデータ読み込み\n",
    "SparkでJsonのデータを読み込んでみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jsonデータの読み込み\n",
    "json_df=spark.read.json(\"./dataset/jinko.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSVのデータ読み込み\n",
    "CSV(TSV)の読み込みは非常にパターンが多いです。\n",
    "ひとつを例にとってパターンを見てみましょう。\n",
    "\n",
    "多すぎるので全て、紹介できませんがいくつかオプションをみていきましょう。\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-csv.html\n",
    "\n",
    "- inferSchema(型をデータの中から推論するかどうか？推論なのでデータの中身によって毎度変わる可能性があるので注意)\n",
    "- lineSep(sep),(csvであれば','でtsvであればタブ)\n",
    "- header(一行目をカラム行としてみなすかどうか？)\n",
    "- multiline(カラムで改行されたりした場合に、それを一行としてみるかどうか)\n",
    "- Schema(スーキーマを設定する)\n",
    "- encoding(読み込むエンコードを設定する)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データソースの読み込み\n",
    "#sep='\\t'とすればtsvでも読み込みが可能です\n",
    "#multiLineは、CSVやTSVの各カラムに改行が含まれていた時の対策です。\n",
    "df=spark.read.option(\"multiLine\", \"true\").option(\"encoding\", \"SJIS\").csv(\"/Users/yuki/pyspark_batch/dataset/jinko.csv\", header=True, sep=',', inferSchema=False)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#スキーマ設定をしていきましょう\n",
    "struct = StructType([\n",
    "    StructField(\"code\", StringType(), False),\n",
    "    StructField(\"kenmei\", StringType(), False),\n",
    "    StructField(\"gengo\", StringType(), False),\n",
    "    StructField(\"wareki\", StringType(), False),\n",
    "    StructField(\"seireki\", StringType(), False),\n",
    "    StructField(\"chu\", StringType(), False),\n",
    "    StructField(\"sokei\", StringType(), False),\n",
    "    StructField(\"jinko_male\", StringType(), False),\n",
    "    StructField(\"jinko_female\", StringType(), False)\n",
    "])\n",
    "\n",
    "df_csv=spark.read.option(\"multiLine\", \"true\").option(\"encoding\", \"SJIS\") \\\n",
    "    .csv(\"/home/pyspark/pyspark_super_crush_course/dataset/jinko.csv\", header=False, sep=',', inferSchema=False, schema=struct)\n",
    "\n",
    "df_csv.show(truncate=False,n=4)\n",
    "\n",
    "# headerやsepなどはoptionで渡すことも可能ですが、csv関数の引数として渡すことも可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データフレーム\n",
    "df_csvやjson_dfがデータフレームと呼ばれる変数\n",
    "\n",
    "データを枠組み(フレーム)として扱うことができるのがデータフレームです。  \n",
    "データフレームの作成方法は\n",
    "\n",
    "- SQLから生成する\n",
    "- データを読み込んで生成する\n",
    "- プログラム的に作成する\n",
    "\n",
    "SQLから生成はSQLで取得した結果がデータフレームで返却されるます  \n",
    "\n",
    "データを読み込んで生成する場合はCSVやJSon、Parquet、Avroなどのファイルを読み込んでデータフレームに取り込みを行うことが可能  \n",
    "\n",
    "プログラム的に作成するのは、空のデータフレームが欲しい時など\n",
    "\n",
    "## データフレームの操作\n",
    "データフレームに対する操作はたくさんあるのですが、今回はよく使う\n",
    "\n",
    "- withColumns\n",
    "- When\n",
    "- null関係の操作(fillna)\n",
    "\n",
    "について紹介していきたいと思います。\n",
    "\n",
    "集計関数については別のセクションで紹介します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実演する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# カラムナーフォーマット\n",
    "ここまでで読み込んだJsonやCsvはローデータとよばれる、データ基盤では扱いづらい部類に入るファイルです。  \n",
    "扱いづらい理由はファイルがスプリッタブルかどうか？という点が関わっています。  \n",
    "\n",
    "スプリッタブルであればデータを複数端末で分割して処理することが可能ですし、そうでなければ分割して処理をすることができないため非効率が発生します。  \n",
    "\n",
    "カラムナーフォーマットと次のレクチャーで紹介するAvro(行指向フォーマット)はいずれもスプリッタブルなファイルです。  \n",
    "そのため、CSVやJSON形式のままデータ基盤で活用を続けるのは悪手なので、必ずカラムナーフォーマット、行指向フォーマットへ変換する処理を挟みましょう。  \n",
    "\n",
    "カラムナーフォーマットとは、分析に特化したファイル形式です。  \n",
    "列ごとにデータをまとめて保存することによって、データを圧縮し検索速度(特にGroup byなどの集計構文)をあげています。  \n",
    "また、カラムナーフォーマットは内部にカラム情報を保持しているためセクション2で紹介したスキーマオンリード機能を使ってスキーマ定義を読み出すことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 圧縮形式とファイルフォーマット\n",
    "\n",
    "前のレクチャーではスプリッタブルかどうかが重要であるとお伝えしました。  \n",
    "ここで一度、ファイルと圧縮形式の組み合わせについてみていきましょう。\n",
    "\n",
    "```\n",
    "        paruqet avro csv/json\n",
    "gz       Y      Y      N\n",
    "snappy   Y      Y      -\n",
    "bz2      -      -      Y\n",
    "圧縮なし  Y      Y      N\n",
    "\n",
    "```\n",
    "\n",
    "大事なのは、csvやjsonの場合はbz2を利用しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 行指向フォーマット\n",
    "先ほどはカラムナーフォーマットを紹介しましたが、次は行指向フォーマットを紹介について紹介していきます。  \n",
    "\n",
    "よく利用されているRDBも行指向の仕組みを持ったデータシステムです。  \n",
    "RDBはレコードの追加が得意です。  \n",
    " \n",
    "そして、ビッグデータはレコードを一件つづ追加していく様な処理が苦手でしたが行指向フォーマットのAvroの登場によってレコードの追加が頻繁に発生する処理  \n",
    "つまりストリーミング処理にも適用することが可能になりました。  \n",
    "\n",
    "まず覚えてほしいのは、Avroはメインとしてストリーミング処理の利用に向いているということです（バッチ処理にも使うことが可能です）  \n",
    "\n",
    "\n",
    "ビッグデータにおけるストリーミングについて詳しく知りたい方は  \n",
    "「データサイエンスのためのストリーミング前処理入門　PythonとSparkで始めるビッグデータストリーミング処理入門」\n",
    "を是非受講ください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#　パーティションとダイナミックパーティション\n",
    "\n",
    "データはパーティションと呼ばれれる区切りにデータを保存していくことが可能です。\n",
    "\n",
    "```\n",
    "テーブル\n",
    "    パーティション1\n",
    "        パーティション1に関するデータ\n",
    "\n",
    "    パーティション2\n",
    "        パーティション2に関するデータ\n",
    "```\n",
    "\n",
    "今回は、パーティションとしてkenmeiを利用して保存をしていこうと思います。\n",
    "また、ファイルの出力としてParquet/Avroでそれぞれ出力を行ってみたいと思います。\n",
    "\n",
    "## ダイナミックパーティション\n",
    "ダイナミックパーティションとはデータをもとにパーティションを作成することを指します。  \n",
    "Hiveなどで使われる言葉ですが、データをもとに振り分けるんだな。  \n",
    "とだけ理解しておけばいいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartitionはパーティション配下の\n",
    "# パーティションなしの場合\n",
    "\n",
    "# パーティションありの場合\n",
    "\n",
    "# Avroでファイルを出力してみよう\n",
    "avro_file.repartition(2).write.mode('overwrite').format(\"avro\").save(\"/tmp/avro_etl/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output のパス\n",
    "/home/pyspark/super_crush_course.db/parquet_table_with_partition\n",
    "/home/pyspark/super_crush_course.db/parquet_table_with_no_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "578f5f657c2fb65ecadb997ad60e5cf2da380ecec34305a6dd913dc5b96e257c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
